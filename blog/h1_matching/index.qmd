---
title: "A Replication of Karlan and List (2007)"
author: "Your Name"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
jupyter: python3
---


## Introduction

Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

The experiment by Karlan and List was designed to test how different types of matching donations affect individuals' likelihood to give. Using a large-scale natural field experiment, they sent fundraising letters to over 50,000 previous donors of a politically oriented nonprofit organization. These letters were randomly assigned to different treatment groups, with variations in the **match ratio** ($1:$1, $2:$1, $3:$1), **match cap** ($25K, $50K, $100K, or unstated), and **suggested donation amount** (based on prior giving).

The results showed that offering a matching donation significantly increased both response rates and donation amounts, but surprisingly, **higher match ratios did not yield higher contributions**. Furthermore, the treatment effects were found to be **stronger in politically conservative (red) states**.

This replication will use the provided dataset to reproduce the main findings of their study and visualize key trends from the original experiment.

This project seeks to replicate their results.


## Data

### Description

This dataset includes 50,083 observations from a field experiment conducted by Karlan and List (2007). Each observation represents a previous donor to a liberal nonprofit organization in the U.S., who received a fundraising letter with randomized treatments. The variables cover experimental assignments (e.g., matching ratios and amounts), donation responses, and background characteristics such as donation history, gender, and zip-codeâ€“level census data.

```{python}
#| echo: false
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_stata("karlan_list_2007.dta")
```

::: {.callout-note collapse="true"}
### ðŸ“‹ Overview of Variables

```{python}
#| echo: false
summary_table = pd.DataFrame({
    "Variable": df.columns,
    "Data Type": df.dtypes.astype(str),
    "Missing (%)": (df.isnull().mean() * 100).round(2)
})
summary_table
```
:::


::: {.callout-note collapse="true"}
### ðŸ“ˆ Summary Statistics for Key Variables

```{python}
#| echo: false
key_vars = ["treatment", "control", "ratio2", "ratio3", "size25", "size50", "size100", "sizeno", "amount"]
df[key_vars].describe().T.round(2)
```
:::


::: {.callout-note collapse="true"}
### ðŸ“Š Donation Amount Distributions

```{python}
#| echo: false
import matplotlib.pyplot as plt

# Figure 1: Positive Donor Distribution
plt.figure(figsize=(6, 4))
df[df["amount"] > 0]["amount"].plot(kind="hist", bins=30, edgecolor="black")
plt.title("Distribution of Donation Amounts (Positive Donations Only)")
plt.xlabel("Donation Amount ($)")
plt.ylabel("Number of Donors")
plt.tight_layout()
plt.show()
```

**Figure 1**: *This histogram shows the distribution of donation amounts for individuals who gave a positive amount. The majority of donations are relatively small â€” mostly under $50 â€” with a sharp decline in frequency as the donation amount increases. A small number of donors gave over $100, but such large donations are rare.*

---

```{python}
#| echo: false

# Figure 2: Logarithmic scale showing all donations (including 0)
plt.figure(figsize=(6, 4))
df[df["amount"] <= 100]["amount"].plot(kind="hist", bins=30, edgecolor="black")
plt.yscale("log")
plt.title("Distribution of Donation Amounts (Log Scale, â‰¤ $100)")
plt.xlabel("Donation Amount ($)")
plt.ylabel("Log(Number of Donors)")
plt.tight_layout()
plt.show()
```

**Figure 2**: *This figure uses a logarithmic y-axis to better visualize the heavily right-skewed distribution of donation amounts, including those who gave $0. It highlights that most donors either gave nothing or made small contributions (typically under $20), while only a few gave larger amounts. The log scale allows us to observe variation across the full range despite the large number of zero or near-zero donations.*
:::




:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::


### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.

As a check on the randomization mechanism, we compare several pre-treatment covariates between the treatment and control groups. We use **t-tests** and **linear regressions** to test whether the differences are statistically significant.

### T-Test Results for Selected Variables

```{python}
#| echo: false
import scipy.stats as stats
import pandas as pd

# List of pre-processing variables
balance_vars = ['years', 'freq', 'female', 'couple', 'hpa', 'amountchange']

# t-test
t_test_results = {}
for var in balance_vars:
    t_stat, p_val = stats.ttest_ind(
        df[df["treatment"] == 1][var].dropna(),
        df[df["treatment"] == 0][var].dropna(),
        equal_var=False
    )
    t_test_results[var] = {"t-statistic": round(t_stat, 3), "p-value": round(p_val, 4)}

t_test_df = pd.DataFrame.from_dict(t_test_results, orient="index")
t_test_df.index.name = "Variable"
t_test_df
```
::: {.callout-note}
**Interpretation**: None of the selected variables show statistically significant differences between treatment and control groups at the 5% level. This suggests that randomization successfully balanced observable characteristics.
::: 


### Balance Check via Linear Regression

```{python}
#| echo: false
import statsmodels.formula.api as smf

# Linear regression test:
reg_results = {}
for var in balance_vars:
    model = smf.ols(f"{var} ~ treatment", data=df).fit()
    reg_results[var] = {
        "Treatment Coef": round(model.params["treatment"], 3),
        "p-value": round(model.pvalues["treatment"], 4)
    }

reg_df = pd.DataFrame.from_dict(reg_results, orient="index")
reg_df.index.name = "Variable"
reg_df
```
::: {.callout-note}
**Interpretation**: Consistent with the t-tests, none of the regression coefficients for `treatment` are statistically significant. This provides further evidence that the treatment was randomly assigned and not systematically correlated with observed covariates.
::: 

**Why include this in the paper?**  
This type of balance tableâ€”commonly shown as Table 1 in field experiment papersâ€”helps build confidence in the internal validity of the study. It reassures readers that any observed treatment effects are likely due to the intervention, not pre-existing differences.



## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

We examine whether offering a matching grant increases the likelihood of making a charitable donation.

---

#### 1. Barplot â€“ Proportion of People Who Donated

```{python}
#| echo: false
#| code-fold: true
import seaborn as sns
import matplotlib.pyplot as plt

# Create a binary variable for donating
df["donated"] = df["amount"] > 0

# Compute donation rates
donation_rate = df.groupby("treatment")["donated"].mean().reset_index()
donation_rate["Group"] = donation_rate["treatment"].map({1: "Treatment", 0: "Control"})

# Plot
plt.figure(figsize=(6, 4))
sns.barplot(data=donation_rate, x="Group", y="donated")
plt.ylabel("Proportion Donated")
plt.xlabel("")
plt.title("Donation Rate by Treatment Group")
plt.ylim(0, 0.05)
plt.tight_layout()
plt.show()
```
::: {.callout-note}
**Interpretation**: The treatment group had a higher donation rate than the control group. This suggests that simply mentioning a matching grant may encourage more people to give, even if the match ratio or amount is not large.
:::

---

#### 2. T-Test and Linear Regression

```{python}
#| echo: false
#| code-fold: true
from scipy.stats import ttest_ind
import statsmodels.formula.api as smf

df["donated"] = df["donated"].astype(int)

# T-test
t_stat, p_val = ttest_ind(df[df["treatment"] == 1]["donated"],
                          df[df["treatment"] == 0]["donated"],
                          equal_var=False)

# Show formatted result
pd.DataFrame({
    "Test": ["T-test"],
    "t-statistic": [round(t_stat, 3)],
    "p-value": [round(p_val, 4)]
})
```

```{python}
#| echo: false
#| code-fold: true
# Linear regression (OLS)
ols_model = smf.ols("donated ~ treatment", data=df).fit()
ols_model.summary2().tables[1]
```
::: {.callout-note}
**Interpretation**: The t-test shows a statistically significant difference in donation rates between the treatment and control groups. The linear regression confirms this, with the `treatment` coefficient being positive and significant. These results replicate the finding in Table 2a (Panel A) of the original paper: match offers increase the likelihood of giving.
:::
*This corresponds to Table 2a, Panel A of Karlan & List (2007), where the estimated treatment effect on donation probability is also positive and statistically significant.*

---

#### 3. Probit Regression (Replicating Table 3, Column 1)

```{python}
#| echo: false
#| code-fold: true
import statsmodels.api as sm

# Probit regression
probit_model = smf.probit("donated ~ treatment", data=df).fit()
probit_model.summary2().tables[1]
```
::: {.callout-note}
**Interpretation**: The probit regression shows a positive and significant coefficient on the treatment variable. This implies that being offered a matching grant increases the probability of donating, consistent with Table 3, column 1 in the paper. The result further supports the idea that behavioral cuesâ€”like matching fundsâ€”can meaningfully influence charitable behavior.
::: 
*These results replicate Table 3, Column 1 in Karlan & List (2007), which shows that treatment assignment increases the likelihood of donating.*
 


### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.
*Assessing the effect of different matching ratios (1:1 vs 2:1 vs 3:1) on donation likelihood*

---

#### 1. Visual Comparison of Donation Rates by Match Ratio
```{python}
#| echo: false
#| code-fold: true
from scipy.stats import ttest_ind
import pandas as pd

# donated as binary (ensure 0/1)
df["donated"] = (df["amount"] > 0).astype(int)

# Get baseline (1:1) group: ratio2==0 and ratio3==0
base_group = df[(df["ratio2"] == 0) & (df["ratio3"] == 0)]["donated"]

# T-tests: 2:1 vs 1:1, and 3:1 vs 1:1
t2 = ttest_ind(df[df["ratio2"] == 1]["donated"], base_group, equal_var=False)
t3 = ttest_ind(df[df["ratio3"] == 1]["donated"], base_group, equal_var=False)

pd.DataFrame({
    "Comparison": ["2:1 vs 1:1", "3:1 vs 1:1"],
    "t-stat": [round(t2.statistic, 3), round(t3.statistic, 3)],
    "p-value": [round(t2.pvalue, 4), round(t3.pvalue, 4)]
})
```
```{python}
#| echo: false
#| code-fold: true
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare labels for match ratio groups
df["match_ratio"] = df.apply(
    lambda row: "1:1" if row["ratio2"] == 0 and row["ratio3"] == 0 else
                "2:1" if row["ratio2"] == 1 else
                "3:1", axis=1
)

# Compute donation rate per group
rate_df = df.groupby("match_ratio")["donated"].mean().reset_index()

# Plot
plt.figure(figsize=(6, 4))
sns.barplot(x="match_ratio", y="donated", data=rate_df)
plt.ylabel("Donation Rate")
plt.xlabel("Match Ratio")
plt.title("Donation Rate by Match Ratio")
plt.ylim(0, 0.03)
plt.tight_layout()
plt.show()
```
::: {.callout-note}
**Interpretation**: Neither the 2:1 nor 3:1 match ratios produced a statistically significant increase in the likelihood of donating compared to the 1:1 ratio. This is consistent with the authorsâ€™ comment that "figures suggest" thereâ€™s no meaningful difference among match sizes. The barplot confirms that the donation rates are very similar across all match ratio groups, supporting the conclusion that increasing the match ratio does not substantially increase giving.
::: 
---

#### 2. Linear Regression with Match Ratio Indicators

```{python}
#| echo: false
#| code-fold: true
import statsmodels.formula.api as smf

model = smf.ols("donated ~ ratio2 + ratio3", data=df).fit()
model.summary2().tables[1]
```
::: {.callout-note}
**Interpretation**:Although the regression coefficients for `ratio2` and `ratio3` are statistically significant, their magnitudes are very small. This suggests that the increase in matching ratio may have a detectable effect, but not one that is economically meaningful. A 2:1 or 3:1 match offer appears no more motivating than a 1:1 match.  
*This regression structure corresponds to Table 2a in Karlan & List (2007).*
::: 
---

#### 3. Calculate Differences in Predicted Probabilities

```{python}
#| echo: false
#| code-fold: true

# Predict mean donation probability by match ratio group
group_means = df.groupby(["ratio2", "ratio3"])["donated"].mean().reset_index()

# add lable
group_means["Match Ratio"] = group_means.apply(
    lambda row: "1:1" if row["ratio2"]==0 and row["ratio3"]==0
    else "2:1" if row["ratio2"]==1
    else "3:1", axis=1
)

# rename donated column
group_means = group_means.rename(columns={"donated": "Donation Rate"})

# show
group_means[["Match Ratio", "Donation Rate"]]
```
::: {.callout-note}
**Interpretation**: The average donation rates across 1:1, 2:1, and 3:1 groups are nearly identical, with differences under 1 percentage point. This supports the conclusion that higher matching ratios do not meaningfully increase donor participation.
*This regression structure corresponds to the estimation in Table 2a of Karlan & List (2007).*
::: 


### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.

#### 1. Linear Regression on Full Sample

```{python}
#| echo: false
#| code-fold: true
df["donated"] = (df["amount"] > 0).astype(int)

import statsmodels.formula.api as smf

full_ols = smf.ols("amount ~ treatment", data=df).fit()
full_ols.summary2().tables[1]
```
::: {.callout-note}
**Interpretation**: On the full sample, we find that treatment has a positive and statistically significant effect on donation amounts. However, this is driven by an increase in the likelihood of donating rather than an increase in how much people give once they choose to donate.
::: 

---

#### 2. Regression on Positive Donors Only

```{python}
#| echo: false
#| code-fold: true
positive_df = df[df["amount"] > 0]
cond_ols = smf.ols("amount ~ treatment", data=positive_df).fit()
cond_ols.summary2().tables[1]
```
::: {.callout-note}
**Interpretation**: Among those who donated, the treatment effect remains positive but is much smaller. This suggests that the match incentive primarily works by encouraging more people to give, not by significantly increasing donation size. Causality is still difficult to assert due to possible selection on unobservables among donors.
::: 

---

#### 3. Histogram of Donation Amounts (Conditioned on Giving)

```{python}
#| echo: false
#| code-fold: true
import matplotlib.pyplot as plt

# Split groups
treatment_group = positive_df[positive_df["treatment"] == 1]["amount"]
control_group = positive_df[positive_df["treatment"] == 0]["amount"]

# Plot
plt.figure(figsize=(8, 3))

# Control
plt.subplot(1, 2, 1)
plt.hist(control_group, bins=30, color="skyblue", edgecolor="black")
plt.axvline(control_group.mean(), color="red", linestyle="--", label=f"Mean = {control_group.mean():.2f}")
plt.title("Control Group â€“ Donation Amounts")
plt.xlabel("Amount ($)")
plt.ylabel("Number of Donors")
plt.legend()

# Treatment
plt.subplot(1, 2, 2)
plt.hist(treatment_group, bins=30, color="lightgreen", edgecolor="black")
plt.axvline(treatment_group.mean(), color="red", linestyle="--", label=f"Mean = {treatment_group.mean():.2f}")
plt.title("Treatment Group â€“ Donation Amounts")
plt.xlabel("Amount ($)")
plt.ylabel("Number of Donors")
plt.legend()

plt.tight_layout()
plt.show()
```
::: {.callout-note}
**Interpretation**: The histograms show that both treatment and control groups follow a similar donation distribution, with only a small difference in average donation amounts. This visual reinforces the regression results: treatment primarily increases the chance of giving, not the donation size.
:::

::: {.callout-tip}
#### Summary: What Did We Learn?

- On the full sample, treatment increases donation amount â€” but this is because more people donate, not because they give more.
- Among those who donated, the treatment has no statistically significant impact on how much they gave.
- Visual distributions reinforce this: both groups have similar donation patterns once giving is triggered.
- In short, **the match incentive primarily influences whether people give, not how much they give**.
:::


## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers

```{python}
#| echo: false
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

p_control = 0.018
p_treatment = 0.022
n = 10000

# Simulate
control_draws = np.random.binomial(1, p_control, n)
treat_draws = np.random.binomial(1, p_treatment, n)
cum_diff = np.cumsum(treat_draws - control_draws) / np.arange(1, n + 1)

# Plot
plt.figure(figsize=(8, 4))
plt.plot(cum_diff, color='orange', label="Cumulative Mean Difference")
plt.axhline(p_treatment - p_control, color='red', linestyle='--', label="True Difference")
plt.title("Law of Large Numbers: Cumulative Mean Difference")
plt.xlabel("Simulation Iteration")
plt.ylabel("Cumulative Difference")
plt.legend()
plt.tight_layout()
plt.show()
```

::: {.callout-note}
**Interpretation â€“ LLN**:  
This simulation demonstrates the Law of Large Numbers. As the number of simulated observations increases, the cumulative difference between the treatment and control groups quickly stabilizes around the true value (0.004). With a fixed random seed, we observe smoother convergence behavior, reinforcing that with large enough samples, empirical means converge to theoretical expectations.
:::

---


### Central Limit Theorem

```{python}
#| echo: false
#| code-fold: true
import seaborn as sns

p_c, p_t = 0.018, 0.022
sample_sizes = [50, 200, 500, 1000]
reps = 1000

plt.figure(figsize=(8, 6))

for i, n in enumerate(sample_sizes):
    mean_diffs = []
    for _ in range(reps):
        c = np.random.binomial(1, p_c, n)
        t = np.random.binomial(1, p_t, n)
        mean_diffs.append(np.mean(t) - np.mean(c))
    
    plt.subplot(2, 2, i + 1)
    sns.histplot(mean_diffs, bins=30, kde=True, color="purple")
    plt.axvline(0, color="black", linestyle="--", label="Zero")
    plt.title(f"Sample Size = {n}")
    plt.xlabel("Mean Difference")
    plt.ylabel("Frequency")
    plt.legend()

plt.suptitle("Central Limit Theorem: Sampling Distribution of Mean Differences", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()
```

::: {.callout-note}
**Interpretation â€“ CLT**:  
Each histogram shows the distribution of 1,000 simulated mean differences between treatment and control groups at different sample sizes. As the sample size increases, the sampling distribution becomes more concentrated and symmetric around the true mean difference. This is consistent with the Central Limit Theorem.
:::


## Conclusion
In this project, I replicate and explore findings from Karlan and List (2007), who conducted a large-scale natural field experiment to test whether and how charitable giving responds to matching donations.

My analysis confirms their key result: simply offering a matching grant significantly increases the likelihood that an individual donates. However, consistent with the paper's findings, I also find that increasing the match ratio beyond 1:1 (to 2:1 or 3:1) does not lead to higher response rates or larger donations. This suggests that the psychological nudge of a match offerâ€”rather than its financial magnitudeâ€”drives behavior.

Furthermore, while treatment boosts average donation amounts on the full sample, this effect disappears when conditioning on those who gave. This indicates that matching primarily operates on the extensive margin (whether to give), not the intensive margin (how much to give).

Through simulation, I also illustrate the Law of Large Numbers and the Central Limit Theorem, showing how statistical inference allows us to draw valid conclusions from randomized experiments.

Taken together, these results highlight the power of simple behavioral interventionsâ€”like a matching messageâ€”to shape real-world decision making. At the same time, they reinforce the importance of rigorous experimental design and replication for understanding causal effects in charitable and policy-relevant settings.


