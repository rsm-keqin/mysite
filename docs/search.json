[
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Resume",
    "section": "",
    "text": "üìç San Diego, USA | üìû +1 858-281-3806 | ‚úâÔ∏è qinkh1009@gmail.com\n\n\n\nUniversity of California, San Diego\nM.S. in Business Analytics ‚Äì Rady School of Management\nJul 2024 ‚Äì Jun 2025\nUniversity of Melbourne\nB.Com. in Economics\nMar 2021 ‚Äì Feb 2024\nUniversity of Edinburgh (Exchange ‚Äì College of Arts, Humanities, and Social Sciences)\nSep 2023 ‚Äì Dec 2023\n\n\n\n\nYicun Capital Co., Ltd. ‚Äì Project Management Intern\nShanghai | Mar 2024 ‚Äì Jun 2024\n\nConducted preliminary evaluations for potential equity investment projects, assessing financial performance, market prospects, and competitive landscape to determine investment potential.\nProvided data-driven insights and analysis to support management‚Äôs decision-making on whether to proceed with further due diligence.\nConducted in-depth research on key industries, such as the MR hardware sector, tracking technological advancements, market dynamics, and competitive landscape.\n\nAlibaba Consulting ‚Äì Market Research Intern\nRemote | Nov 2022 ‚Äì Feb 2023\n\nConducted data mining and consumer behavior analysis in the retail and pharmaceutical industries to support targeted marketing and user experience optimization.\nApplied SPSS, MATLAB, and linear regression models to forecast pharmaceutical company performance, assisting in strategic planning and decision-making.Delivered data-driven reports on market trends, product performance, and financial evaluations to optimize business and investment strategies.\n\n\n\n\n\nCreative Gaming Marketing Optimization (Feb 2025)\n\nDesigned and executed A/B tests, leveraging data analysis to refine targeting strategies and improve ad performance.\nDeveloped and compared Uplift and Propensity models to predict users‚Äô purchase intent based on advertisement placement, increasing purchase intention rates.\nConducted modeling analysis using Pyrsm with logistic regression, neural networks, random forests, and XGBoost, evaluating model effectiveness through AUC, incremental profit calculation, and other key metrics.\n\nIntuit QuickBooks Upgrade Campaign (Jan 2025)\n\nDeveloped an optimized email marketing strategy using Logistic Regression and Neural Network models, using AUC, ROME, and profitability calculations to predict which businesses are most likely to purchase upgrades in second email campaigns.\nCollaborated with marketing and product teams to refine customer segmentation and targeting, improving precision in customer outreach. Provided data-driven decision support for target group selection, optimizing marketing effectiveness and corporate profitability.\n\nAmazon Video Game Ratings Analysis (Dec 2024)\n\nLed a project analyzing 27 years (1996‚Äì2023) of Amazon customer reviews to predict video game ratings, leveraging data collected by McAuley Lab.\nCleaned and processed raw data, filtering for meaningful features such as price, platform source (Nintendo, PlayStation, Xbox), review text length, and sentiment indicators.\nDeveloped and tested multiple models, including baseline, bias, and linear regression models, to predict user ratings, achieving optimal results with linear regression while minimizing overfitting.\nIdentified key factors influencing customer sentiment and product satisfaction, created a market recommendation report to provide actionable insights for game developers.\n\n\n\n\n\nAWS Industry Research Competition ‚Äì Winner\nApr 2022\n\nLed a 4-member team to complete a comprehensive industry research report within a month, successfully presenting findings in an online pitch and winning the final competition.\nConducted a macro-level analysis of the renewable energy industry, evaluating global policies on carbon neutrality, market demand for lithium-ion batteries, and the overall industry value chain.Analyzed battery cost reduction strategies by comparing CATL and BYD‚Äôs battery designs and supplier strategies.h\n\nJacaranda Stock Association ‚Äì Research Leader\nAug 2022 ‚Äì Dec 2023\n\nOrganized and managed industry research competitions, enhancing team collaboration and presentation skills.\nConducted an in-depth analysis of Federal Reserve interest rate hikes, evaluating their impact on the macroeconomy, financial markets, and commodity prices, and presented findings in weekly meetings.\n\n\n\n\n\n\nTools: Python, SQL, Tableau, R, Excel, SPSS\n\nMethods: A/B Testing, Propensity Modeling, Business Analytics\n\nLanguages: Native Mandarin | Fluent English"
  },
  {
    "objectID": "resume/index.html#education",
    "href": "resume/index.html#education",
    "title": "Resume",
    "section": "",
    "text": "University of California, San Diego\nM.S. in Business Analytics ‚Äì Rady School of Management\nJul 2024 ‚Äì Jun 2025\nUniversity of Melbourne\nB.Com. in Economics\nMar 2021 ‚Äì Feb 2024\nUniversity of Edinburgh (Exchange ‚Äì College of Arts, Humanities, and Social Sciences)\nSep 2023 ‚Äì Dec 2023"
  },
  {
    "objectID": "resume/index.html#work-experience",
    "href": "resume/index.html#work-experience",
    "title": "Resume",
    "section": "",
    "text": "Yicun Capital Co., Ltd. ‚Äì Project Management Intern\nShanghai | Mar 2024 ‚Äì Jun 2024\n\nConducted preliminary evaluations for potential equity investment projects, assessing financial performance, market prospects, and competitive landscape to determine investment potential.\nProvided data-driven insights and analysis to support management‚Äôs decision-making on whether to proceed with further due diligence.\nConducted in-depth research on key industries, such as the MR hardware sector, tracking technological advancements, market dynamics, and competitive landscape.\n\nAlibaba Consulting ‚Äì Market Research Intern\nRemote | Nov 2022 ‚Äì Feb 2023\n\nConducted data mining and consumer behavior analysis in the retail and pharmaceutical industries to support targeted marketing and user experience optimization.\nApplied SPSS, MATLAB, and linear regression models to forecast pharmaceutical company performance, assisting in strategic planning and decision-making.Delivered data-driven reports on market trends, product performance, and financial evaluations to optimize business and investment strategies."
  },
  {
    "objectID": "resume/index.html#projects-leadership",
    "href": "resume/index.html#projects-leadership",
    "title": "Resume",
    "section": "",
    "text": "Creative Gaming Marketing Optimization (Feb 2025)\n\nDesigned and executed A/B tests, leveraging data analysis to refine targeting strategies and improve ad performance.\nDeveloped and compared Uplift and Propensity models to predict users‚Äô purchase intent based on advertisement placement, increasing purchase intention rates.\nConducted modeling analysis using Pyrsm with logistic regression, neural networks, random forests, and XGBoost, evaluating model effectiveness through AUC, incremental profit calculation, and other key metrics.\n\nIntuit QuickBooks Upgrade Campaign (Jan 2025)\n\nDeveloped an optimized email marketing strategy using Logistic Regression and Neural Network models, using AUC, ROME, and profitability calculations to predict which businesses are most likely to purchase upgrades in second email campaigns.\nCollaborated with marketing and product teams to refine customer segmentation and targeting, improving precision in customer outreach. Provided data-driven decision support for target group selection, optimizing marketing effectiveness and corporate profitability.\n\nAmazon Video Game Ratings Analysis (Dec 2024)\n\nLed a project analyzing 27 years (1996‚Äì2023) of Amazon customer reviews to predict video game ratings, leveraging data collected by McAuley Lab.\nCleaned and processed raw data, filtering for meaningful features such as price, platform source (Nintendo, PlayStation, Xbox), review text length, and sentiment indicators.\nDeveloped and tested multiple models, including baseline, bias, and linear regression models, to predict user ratings, achieving optimal results with linear regression while minimizing overfitting.\nIdentified key factors influencing customer sentiment and product satisfaction, created a market recommendation report to provide actionable insights for game developers."
  },
  {
    "objectID": "resume/index.html#research-experience",
    "href": "resume/index.html#research-experience",
    "title": "Resume",
    "section": "",
    "text": "AWS Industry Research Competition ‚Äì Winner\nApr 2022\n\nLed a 4-member team to complete a comprehensive industry research report within a month, successfully presenting findings in an online pitch and winning the final competition.\nConducted a macro-level analysis of the renewable energy industry, evaluating global policies on carbon neutrality, market demand for lithium-ion batteries, and the overall industry value chain.Analyzed battery cost reduction strategies by comparing CATL and BYD‚Äôs battery designs and supplier strategies.h\n\nJacaranda Stock Association ‚Äì Research Leader\nAug 2022 ‚Äì Dec 2023\n\nOrganized and managed industry research competitions, enhancing team collaboration and presentation skills.\nConducted an in-depth analysis of Federal Reserve interest rate hikes, evaluating their impact on the macroeconomy, financial markets, and commodity prices, and presented findings in weekly meetings."
  },
  {
    "objectID": "resume/index.html#skills-certifications",
    "href": "resume/index.html#skills-certifications",
    "title": "Resume",
    "section": "",
    "text": "Tools: Python, SQL, Tableau, R, Excel, SPSS\n\nMethods: A/B Testing, Propensity Modeling, Business Analytics\n\nLanguages: Native Mandarin | Fluent English"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nKehang Qin\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nKehang\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nKehang Qin\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifying Key Drivers of Customer Satisfaction: A Comparative Modeling Approach\n\n\n\n\n\n\nKehang Qin\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProject 1\n\n\n\n\n\n\n\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/h4_matching/index.html",
    "href": "blog/h4_matching/index.html",
    "title": "Identifying Key Drivers of Customer Satisfaction: A Comparative Modeling Approach",
    "section": "",
    "text": "A\n\n\n\n\n\n\n\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can ‚Äúsee‚Äù the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,‚Ä¶,7). What is the ‚Äúright‚Äù number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this.\n\n\n\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\nn = 100\n\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\n\nX = np.column_stack((x1, x2)) \nboundary = np.sin(4 * x1) + x1\ny = np.where(x2 &gt; boundary, 1, 0).astype(str)\ndat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function ‚Äì eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn‚Äôs KNeighborsClassifier in Python.\ntodo: run your function for k=1,‚Ä¶,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot? :::"
  },
  {
    "objectID": "blog/h4_matching/index.html#a.-k-means",
    "href": "blog/h4_matching/index.html#a.-k-means",
    "title": "Identifying Key Drivers of Customer Satisfaction: A Comparative Modeling Approach",
    "section": "",
    "text": "todo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can ‚Äúsee‚Äù the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,‚Ä¶,7). What is the ‚Äúright‚Äù number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "blog/h4_matching/index.html#b.-latent-class-mnl",
    "href": "blog/h4_matching/index.html#b.-latent-class-mnl",
    "title": "Identifying Key Drivers of Customer Satisfaction: A Comparative Modeling Approach",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\n\nReshape yogurt data from wide to long format\nThe original yogurt dataset was provided in a wide format, where each row represented one consumer and contained multiple product-related variables. To facilitate multinomial logit modeling, the data were transformed into long format. In the reshaped structure, each row corresponds to one product alternative per consumer, with associated attributes including price, featured status, and a binary choice indicator.\n\n\nShow code\n# Load yogurt dataset and reshape it from wide to long format\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv(\"yogurt_data.csv\")\n\n# Convert wide format to long format suitable for MNL estimation\ndef reshape_yogurt_data(df):\n    records = []\n    for _, row in df.iterrows():\n        for j in range(1, 5):  # Iterate over 4 yogurt options\n            records.append({\n                \"id\": row[\"id\"],\n                \"alt\": j,\n                \"price\": row[f\"p{j}\"],\n                \"featured\": row[f\"f{j}\"],\n                \"choice\": 1 if row[f\"y{j}\"] == 1 else 0\n            })\n    return pd.DataFrame(records)\n\nlong_df = reshape_yogurt_data(df)\nlong_df.head()\n\n\n\n\n\n\n\n\n\nid\nalt\nprice\nfeatured\nchoice\n\n\n\n\n0\n1.0\n1\n0.108\n0.0\n0\n\n\n1\n1.0\n2\n0.081\n0.0\n0\n\n\n2\n1.0\n3\n0.061\n0.0\n0\n\n\n3\n1.0\n4\n0.079\n0.0\n1\n\n\n4\n2.0\n1\n0.108\n0.0\n0\n\n\n\n\n\n\n\n\n\nFit standard MNL and latent-class MNL (for K=2 to 5)\nA standard Multinomial Logit (MNL) model was estimated using the long-format data. The model includes price, featured advertising status, and alternative-specific intercepts as predictors. Coefficients were estimated using maximum likelihood via logistic regression. As expected, price exerts a negative influence on choice probability, while featured advertising has a positive effect.\nTo account for preference heterogeneity, latent-class MNL models were approximated by fitting Gaussian Mixture Models (GMMs) to the product-level covariates (price and featured). Probabilistic segment assignments were obtained for each observation across model specifications with 2 to 5 latent classes. The log-likelihood of each fitted GMM was recorded for model comparison.\n\n\nShow code\n# Fit standard multinomial logit and latent-class MNLs (K=2,3,4,5)\nimport statsmodels.formula.api as smf\nfrom sklearn.mixture import GaussianMixture\nimport numpy as np\n\n# Fit standard MNL using statsmodels (with alternative-specific intercepts)\nlong_df[\"alt_2\"] = (long_df[\"alt\"] == 2).astype(int)\nlong_df[\"alt_3\"] = (long_df[\"alt\"] == 3).astype(int)\nlong_df[\"alt_4\"] = (long_df[\"alt\"] == 4).astype(int)\n\nmnl_model = smf.logit(\"choice ~ price + featured + alt_2 + alt_3 + alt_4\", data=long_df).fit()\nprint(mnl_model.summary())\n\n# Fit latent-class MNL using GMM clustering as proxy for soft segment assignment\nX_cluster = long_df[[\"price\", \"featured\"]]\nlog_likelihoods = []\nclass_probs = {}\n\nfor k in range(2, 6):  # For K = 2, 3, 4, 5\n    gmm = GaussianMixture(n_components=k, random_state=0)\n    gmm.fit(X_cluster)\n    class_probs[k] = gmm.predict_proba(X_cluster)\n    log_likelihoods.append(gmm.lower_bound_ * len(X_cluster))  # Approximate total log-likelihood\n\n\nOptimization terminated successfully.\n         Current function value: 0.477971\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 9720\nModel:                          Logit   Df Residuals:                     9714\nMethod:                           MLE   Df Model:                            5\nDate:                Wed, 11 Jun 2025   Pseudo R-squ.:                  0.1500\nTime:                        00:19:43   Log-Likelihood:                -4645.9\nconverged:                       True   LL-Null:                       -5465.9\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.7009      0.229     11.795      0.000       2.252       3.150\nprice        -31.9761      2.089    -15.305      0.000     -36.071     -27.881\nfeatured       0.4714      0.119      3.959      0.000       0.238       0.705\nalt_2         -0.5166      0.081     -6.354      0.000      -0.676      -0.357\nalt_3         -4.5584      0.173    -26.319      0.000      -4.898      -4.219\nalt_4         -1.4179      0.089    -16.008      0.000      -1.591      -1.244\n==============================================================================\n\n\n\n\nCalculate BIC and determine the best number of classes\nModel fit was evaluated using the Bayesian Information Criterion (BIC), computed as:\n\\[\nBIC = -2 \\cdot \\ell_n + k \\cdot \\log(n)\n\\]\nwhere ( _n ) is the log-likelihood of the model, ( k ) is the number of estimated parameters, and ( n ) is the sample size. A lower BIC indicates a more favorable balance between model complexity and fit. BIC values were calculated for models with 2 through 5 latent classes.\nThe BIC curve indicates that the model with K = 5 latent classes achieves the lowest BIC value, suggesting it is the most appropriate specification among those considered.\n\n\nShow code\n# Compute BIC values for each latent class model and select the best K\nimport matplotlib.pyplot as plt\n\nn_obs = len(X_cluster)\nbic_scores = []\n\nfor k, logL in zip(range(2, 6), log_likelihoods):\n    n_params = k * 3  # Assume 3 parameters (intercept, price, featured) per class\n    bic = -2 * logL + n_params * np.log(n_obs)\n    bic_scores.append(bic)\n\n# Plot BIC vs number of latent classes\nplt.plot(range(2, 6), bic_scores, marker='o')\nplt.xlabel(\"Number of latent classes (K)\")\nplt.ylabel(\"BIC\")\nplt.title(\"Model Selection by BIC\")\nplt.show()\n\nbest_k = np.argmin(bic_scores) + 2\nprint(f\"Best number of classes suggested by BIC: K = {best_k}\")\n\n\n\n\n\n\n\n\n\nBest number of classes suggested by BIC: K = 5\n\n\n\n\nCompare aggregate MNL vs.¬†latent-class parameter estimates\nTo investigate heterogeneity in price sensitivity, the estimated price coefficient from the aggregate MNL model was compared to the class-weighted average prices implied by the latent-class models. The standard MNL yielded a price coefficient of approximately -31.98, consistent with strong price aversion at the population level.\nFor each latent-class model (K = 2 to 5), the class 1 segment‚Äôs weighted average price was computed. As the number of segments increased, variation in average price levels became more apparent, indicating distinct behavioral patterns across latent classes. These findings highlight the advantage of latent-class MNL in capturing preference heterogeneity that may be obscured in aggregate models.\n\n\nShow code\n# Compare price estimates from aggregate MNL and latent-class segmentation\n# Use class-weighted average price as an intuitive summary\nagg_coef = mnl_model.params[\"price\"]\nprint(f\"Aggregate MNL price coefficient: {agg_coef:.4f}\")\n\n# For each K, compute weighted average price across segments\nfor k in [2, 3, 4, 5]:\n    weights = class_probs[k]\n    weighted_avg_price = np.average(long_df[\"price\"], weights=weights[:, 0])  # just for first segment\n    print(f\"K={k} - Weighted avg price (segment 1): {weighted_avg_price:.4f}\")\n\n\nAggregate MNL price coefficient: -31.9761\nK=2 - Weighted avg price (segment 1): 0.0807\nK=3 - Weighted avg price (segment 1): 0.0696\nK=4 - Weighted avg price (segment 1): 0.0544\nK=5 - Weighted avg price (segment 1): 0.0552\n\n\n\n\nConclusion\nThe latent-class multinomial logit model provides a flexible framework for modeling unobserved consumer heterogeneity. By segmenting the market into latent classes, the model captures variation in preference structures that are not identifiable through aggregate analysis. The five-segment specification was found to provide the best fit according to BIC, and revealed meaningful differences in price sensitivity across segments. This approach offers enhanced insight into consumer choice behavior and may support more targeted pricing and promotional strategies."
  },
  {
    "objectID": "blog/h4_matching/index.html#a.-k-nearest-neighbors",
    "href": "blog/h4_matching/index.html#a.-k-nearest-neighbors",
    "title": "Identifying Key Drivers of Customer Satisfaction: A Comparative Modeling Approach",
    "section": "",
    "text": "todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\nn = 100\n\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\n\nX = np.column_stack((x1, x2)) \nboundary = np.sin(4 * x1) + x1\ny = np.where(x2 &gt; boundary, 1, 0).astype(str)\ndat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})"
  },
  {
    "objectID": "blog/h4_matching/index.html#b.-key-drivers-analysis",
    "href": "blog/h4_matching/index.html#b.-key-drivers-analysis",
    "title": "Identifying Key Drivers of Customer Satisfaction: A Comparative Modeling Approach",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\nThis section replicates the variable importance summary presented in slide 75 of the Session 5 lecture slides. The goal is to identify key predictors of satisfaction using a variety of variable importance metrics. The dataset data_for_drivers_analysis.csv contains ten predictors, and the following five methods were used to evaluate their relative influence:\n\nPearson correlations between each predictor and satisfaction.\nStandardized regression coefficients from a linear model with standardized predictors.\nUsefulness, measured as the reduction in out-of-sample ( R^2 ) when a variable is excluded from the model.\nJohnson‚Äôs relative weights, approximated by the squared standardized coefficients weighted by correlation.\nMean decrease in Gini, computed from a Random Forest model.\n\nAll metrics are scaled to percentages and sorted by average rank across methods to facilitate comparison.\n\n\nShow code\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n# Load data\ndf = pd.read_csv(\"data_for_drivers_analysis.csv\")\nX = df.drop(columns=[\"brand\", \"id\", \"satisfaction\"])\ny = df[\"satisfaction\"]\n\n# Standardize predictors\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\n# Pearson correlations\npearson_corr = X.corrwith(y)\n\n# Standardized regression coefficients\nlr = LinearRegression().fit(X_scaled, y)\nstd_coef = pd.Series(lr.coef_, index=X.columns)\n\n# Usefulness (R¬≤ drop)\nbaseline_r2 = r2_score(y, lr.predict(X_scaled))\nusefulness = pd.Series({\n    col: baseline_r2 - r2_score(y, LinearRegression().fit(X_scaled.drop(columns=[col]), y).predict(X_scaled.drop(columns=[col])))\n    for col in X.columns\n})\n\n# Johnson's relative weights approximation\nrel_weights = std_coef**2 * pearson_corr\n\n# Random Forest Gini importance\nrf = RandomForestRegressor(random_state=42).fit(X, y)\ngini_importance = pd.Series(rf.feature_importances_, index=X.columns)\n\n# XGBoost importance\nxgb = XGBRegressor(random_state=42).fit(X, y)\nxgb_importance = pd.Series(xgb.feature_importances_, index=X.columns)\n\n# Assemble results\nresults = pd.DataFrame({\n    \"Pearson\": pearson_corr,\n    \"Std Coef\": std_coef,\n    \"Usefulness (ŒîR¬≤)\": usefulness,\n    \"Rel Weights\": rel_weights,\n    \"Gini Importance\": gini_importance,\n    \"XGBoost\": xgb_importance\n})\n\n# Format results to percentage scale and rank\nformatted_results = (results * 100).round(1)\nformatted_results[\"Average Rank\"] = formatted_results.rank(ascending=False).mean(axis=1)\nformatted_results = formatted_results.sort_values(\"Average Rank\")\n\nformatted_results\n\n\n\n\n\n\n\n\n\nPearson\nStd Coef\nUsefulness (ŒîR¬≤)\nRel Weights\nGini Importance\nXGBoost\nAverage Rank\n\n\n\n\ntrust\n25.6\n13.6\n0.8\n0.5\n15.6\n29.0\n1.500000\n\n\nimpact\n25.5\n15.0\n1.1\n0.6\n14.1\n18.5\n1.500000\n\n\nservice\n25.1\n10.4\n0.5\n0.3\n13.0\n11.1\n3.000000\n\n\neasy\n21.3\n2.6\n0.0\n0.0\n10.0\n7.1\n6.000000\n\n\nappealing\n20.8\n4.0\n0.1\n0.0\n8.6\n6.6\n6.000000\n\n\nbuild\n19.2\n2.3\n0.0\n0.0\n10.2\n7.9\n6.000000\n\n\ndiffers\n18.5\n3.3\n0.1\n0.0\n9.0\n5.6\n6.833333\n\n\nrewarding\n19.5\n0.6\n0.0\n0.0\n10.1\n6.5\n7.000000\n\n\npopular\n17.1\n1.9\n0.0\n0.0\n9.5\n7.6\n7.166667\n\n\n\n\n\n\n\n\nSummary of Key Driver Rankings\nThe integrated importance table, incorporating six metrics including XGBoost, reveals consistent patterns in identifying the most influential predictors of satisfaction. Two variables‚Äîtrust and impact‚Äîconsistently rank at the top across all metrics, achieving the lowest average rank of 1.5. These results suggest that customers‚Äô trust in the brand and the perceived impact of the brand are the strongest drivers of satisfaction.\nService emerges as the third most important predictor, also scoring highly across linear and tree-based methods. This reinforces the importance of customer service in shaping satisfaction outcomes.\nIn contrast, variables such as popular, rewarding, and differs exhibit low importance across all metrics. Their limited explanatory power suggests they contribute marginally, if at all, to variations in satisfaction in this sample.\nInterestingly, while some predictors (e.g., easy, appealing) show moderate linear correlations, their standardized coefficients and machine learning-based importances (e.g., Gini, XGBoost) remain relatively low, indicating possible redundancy or shared variance with stronger predictors.\nIn summary, the combined analysis across six evaluation methods supports the conclusion that trust, impact, and service are the most robust and consistently influential drivers of customer satisfaction."
  },
  {
    "objectID": "blog/h2_matching/index.html",
    "href": "blog/h2_matching/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head(10)\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n5\n6\nNortheast\n29.5\n1\n\n\n6\n5\nSouthwest\n27.0\n0\n\n\n7\n5\nNortheast\n20.5\n0\n\n\n8\n6\nNortheast\n25.0\n0\n\n\n9\n4\nMidwest\n29.5\n0\n\n\n\n\n\n\n\n\n\n\nTo assess whether Blueprinty customers tend to produce more patents, we examine the distribution and average number of patents awarded over the last five years by customer status.\n\n\nShow code\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", palette=\"Set2\", kde=False)\nplt.title(\"Patent Counts by Customer Status\")\nplt.xlabel(\"Number of Patents (last 5 years)\")\nplt.ylabel(\"Firm Count\")\nplt.tight_layout()\nplt.show()\n\n# Mean patents table\nmean_table = df.groupby(\"iscustomer\")[\"patents\"].mean().reset_index()\nmean_table.columns = [\"Customer Status (0 = No, 1 = Yes)\", \"Average Number of Patents\"]\nmean_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Status (0 = No, 1 = Yes)\nAverage Number of Patents\n\n\n\n\n0\n0\n3.473013\n\n\n1\n1\n4.133056\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation:\nThe histogram reveals that firms using Blueprinty tend to have slightly higher patent productivity. While both customers and non-customers cluster around 2‚Äì5 patents, the customer group has more firms with higher patent counts (e.g., 6+).\nOn average: * Non-customers received approximately 3.47 patents. * Customers received approximately 4.13 patents.\nThis difference, while not conclusive on its own, provides visual and numerical evidence that firms using Blueprinty may be more productive in securing patents. Further analysis (e.g., controlling for region or firm age) is recommended before inferring causality.\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nWe explore whether Blueprinty customers differ systematically in their firm age or region relative to non-customers. If so, this may confound the relationship between customer status and patent output.\n\n\nShow code\n# Region counts\nregion_table = pd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize=\"index\") * 100\nregion_table.columns = [\"Non-customer (%)\", \"Customer (%)\"]\n\n# Age distribution\nsns.kdeplot(data=df, x=\"age\", hue=\"iscustomer\", common_norm=False, fill=True)\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Firm Age (years)\")\nplt.ylabel(\"Density\")\nplt.tight_layout()\nplt.show()\n\nregion_table.round(1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-customer (%)\nCustomer (%)\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.5\n16.5\n\n\nNortheast\n45.4\n54.6\n\n\nNorthwest\n84.5\n15.5\n\n\nSouth\n81.7\n18.3\n\n\nSouthwest\n82.5\n17.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation:\n\nAge: The plot reveals that customers tend to be slightly younger on average than non-customers. This could reflect that newer firms are more likely to adopt modern software like Blueprinty.\nRegion: The table shows differences in customer proportions across regions. Some regions have a higher share of Blueprinty users than others, suggesting region may be a relevant control in further analysis.\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nWe assume that the number of patents ( Y_i ) follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThe log-likelihood function for ( n ) firms is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\n\n\n\n\n\nShow code\nfrom scipy.special import gammaln  \nfrom scipy.optimize import minimize_scalar\n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Compute log-likelihood for Poisson(lmbda) given data y\n    \"\"\"\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ndf = pd.read_csv(\"blueprinty.csv\")\ny_data = df[\"patents\"].values\n\nprint(\"Log-Likelihood at Œª = 4.0:\", poisson_log_likelihood(4.0, y_data))\n\n\nLog-Likelihood at Œª = 4.0: -3386.8380561598083\n\n\n\n\n\nWe evaluate and visualize the log-likelihood function over a range of Œª values and compare the theoretical MLE (sample mean) with the numerically optimized one.\n\n\nShow code\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\ndef poisson_log_likelihood(lmbda, y):\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny_data = df[\"patents\"].values\ny_mean = np.mean(y_data)\n\nlambda_grid = np.linspace(1.0, 7.0, 500)\nloglikelihoods = [poisson_log_likelihood(l, y_data) for l in lambda_grid]\n\nresult = minimize_scalar(lambda l: -poisson_log_likelihood(l, y_data),\n                         bounds=(1.0, 7.0), method=\"bounded\")\nlambda_mle = result.x\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_grid, loglikelihoods, color=\"darkorange\", lw=2, label=\"Log-Likelihood\")\nplt.axvline(y_mean, color=\"blue\", ls=\"--\", label=f\"Sample Mean Œª = {y_mean:.2f}\")\nplt.axvline(lambda_mle, color=\"pink\", ls=\":\", label=f\"Numerical MLE Œª = {lambda_mle:.2f}\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.xlabel(\"Œª (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation:\n\nThe log-likelihood curve reaches its peak near Œª = 3.68, found via numerical optimization.\nThe sample mean of the patent count is ŒªÃÑ = 3.68, which matches the MLE as expected from Poisson theory.\nBecause the numerical and theoretical Œª are essentially equal, both lines overlap on the plot.\nThis confirms that for Poisson models, the MLE is the sample mean, and validates your estimation function.\n\n\n\n\n\n\n\nTo verify the result mathematically, we take the first derivative of the log-likelihood function:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the derivative with respect to ( ) and setting it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThis confirms that the maximum likelihood estimator (MLE) for ( ) in a Poisson model is the sample mean ( {Y} ), which matches our earlier numerical result.\n\n\n\nWe now use numerical optimization to find the Œª that maximizes the Poisson log-likelihood.\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\n# Find Œª that minimizes negative log-likelihood (i.e., maximizes log-likelihood)\nresult = minimize_scalar(\n    lambda l: -poisson_log_likelihood(l, y_data),\n    bounds=(1.0, 7.0),\n    method=\"bounded\"\n)\n\n# Extract results\nlambda_mle = result.x\nloglik_at_mle = -result.fun\n\nprint(f\"Numerical MLE for Œª: {lambda_mle:.4f}\")\nprint(f\"Log-likelihood at MLE: {loglik_at_mle:.2f}\")\n\n\nNumerical MLE for Œª: 3.6847\nLog-likelihood at MLE: -3367.68\n\n\n\n\n\n\n\n\nMLE via Numerical Optimization:\n\nThe numerical MLE for lambda is 3.6847.\n\nThe corresponding log-likelihood value is ‚àí3367.68.\n\nThese values confirm that the MLE aligns with theory: the MLE for a Poisson model equals the sample mean.\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow code\nfrom scipy.special import gammaln\n\ndef poisson_loglik(beta, y, X):\n    beta = np.asarray(beta).reshape(-1)\n    linpred = np.clip(X @ beta, -10, 10)\n    lambdas = np.exp(linpred)\n    return -np.sum(-lambdas + y * linpred - gammaln(y + 1))\n\n\n\n\nShow code\nimport statsmodels.api as sm\n\nX = pd.DataFrame({\n    \"intercept\": 1,\n    \"age\": df[\"age\"],\n    \"age2\": df[\"age\"] ** 2,\n    \"iscustomer\": df[\"iscustomer\"]\n})\nX = pd.get_dummies(X.join(df[\"region\"]), columns=[\"region\"], drop_first=True)\nX_mat = X.astype(float).values\ny = df[\"patents\"].astype(float).values \n\n# GLM model\nglm_model = sm.GLM(y, X_mat, family=sm.families.Poisson())\nglm_result = glm_model.fit()\n\nbeta_hat = glm_result.params\nse_hat = glm_result.bse\n\ncoef_table = pd.DataFrame({\n    \"Covariate\": X.columns,\n    \"Œ≤ÃÇ\": np.round(beta_hat, 4),\n    \"Std. Err\": np.round(se_hat, 4)\n})\n\ncoef_table\n\n\n\n\n\n\n\n\n\nCovariate\nŒ≤ÃÇ\nStd. Err\n\n\n\n\n0\nintercept\n-0.5089\n0.1832\n\n\n1\nage\n0.1486\n0.0139\n\n\n2\nage2\n-0.0030\n0.0003\n\n\n3\niscustomer\n0.2076\n0.0309\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n\n\n6\nregion_South\n0.0566\n0.0527\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n\n\n\n\n\n\n\n\n\n\n\n\n\nGLM Verification:\nTo validate our custom optimization results, we also fit the same Poisson regression model using Python‚Äôs built-in statsmodels.GLM() function.\nThe results match almost exactly, confirming that our custom likelihood function and optimization procedure are correctly implemented.\n\n\n\n\n\n\n\n\n\nInterpretation of Results:\n\nThe coefficient for iscustomer is positive and statistically significant (0.2076, s.e. 0.0309), suggesting that Blueprinty customers are associated with more patent activity.\nBecause the model is log-linear, the effect of being a customer can be interpreted as:\n( e^{0.2076} ) ‚Üí about 23% more patents on average, holding other variables constant.\nCoefficients on age and age¬≤ indicate a concave effect of firm age on patenting.\n\n\n\n\n\n\nShow code\nX_0 = X.copy(); X_0[\"iscustomer\"] = 0\nX_1 = X.copy(); X_1[\"iscustomer\"] = 1\nX_0_mat = X_0.astype(float).values\nX_1_mat = X_1.astype(float).values\n\ny_pred_0 = np.exp(X_0_mat @ beta_hat)\ny_pred_1 = np.exp(X_1_mat @ beta_hat)\n\ntreatment_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Estimated average treatment effect of being a Blueprinty customer:\", round(treatment_effect, 4))\n\n\nEstimated average treatment effect of being a Blueprinty customer: 0.7928\n\n\n\n\n\n\n\n\nCounterfactual Prediction:\nWe use counterfactual prediction to estimate the effect of Blueprinty‚Äôs software.\nBy comparing the predicted number of patents when all firms are treated vs.¬†untreated,\nwe find that the average treatment effect of being a Blueprinty customer is: about 0.79 more patents per firm over 5 years, holding firm characteristics constant."
  },
  {
    "objectID": "blog/h2_matching/index.html#blueprinty-case-study",
    "href": "blog/h2_matching/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head(10)\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n5\n6\nNortheast\n29.5\n1\n\n\n6\n5\nSouthwest\n27.0\n0\n\n\n7\n5\nNortheast\n20.5\n0\n\n\n8\n6\nNortheast\n25.0\n0\n\n\n9\n4\nMidwest\n29.5\n0\n\n\n\n\n\n\n\n\n\n\nTo assess whether Blueprinty customers tend to produce more patents, we examine the distribution and average number of patents awarded over the last five years by customer status.\n\n\nShow code\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", palette=\"Set2\", kde=False)\nplt.title(\"Patent Counts by Customer Status\")\nplt.xlabel(\"Number of Patents (last 5 years)\")\nplt.ylabel(\"Firm Count\")\nplt.tight_layout()\nplt.show()\n\n# Mean patents table\nmean_table = df.groupby(\"iscustomer\")[\"patents\"].mean().reset_index()\nmean_table.columns = [\"Customer Status (0 = No, 1 = Yes)\", \"Average Number of Patents\"]\nmean_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Status (0 = No, 1 = Yes)\nAverage Number of Patents\n\n\n\n\n0\n0\n3.473013\n\n\n1\n1\n4.133056\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation:\nThe histogram reveals that firms using Blueprinty tend to have slightly higher patent productivity. While both customers and non-customers cluster around 2‚Äì5 patents, the customer group has more firms with higher patent counts (e.g., 6+).\nOn average: * Non-customers received approximately 3.47 patents. * Customers received approximately 4.13 patents.\nThis difference, while not conclusive on its own, provides visual and numerical evidence that firms using Blueprinty may be more productive in securing patents. Further analysis (e.g., controlling for region or firm age) is recommended before inferring causality.\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nWe explore whether Blueprinty customers differ systematically in their firm age or region relative to non-customers. If so, this may confound the relationship between customer status and patent output.\n\n\nShow code\n# Region counts\nregion_table = pd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize=\"index\") * 100\nregion_table.columns = [\"Non-customer (%)\", \"Customer (%)\"]\n\n# Age distribution\nsns.kdeplot(data=df, x=\"age\", hue=\"iscustomer\", common_norm=False, fill=True)\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Firm Age (years)\")\nplt.ylabel(\"Density\")\nplt.tight_layout()\nplt.show()\n\nregion_table.round(1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-customer (%)\nCustomer (%)\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.5\n16.5\n\n\nNortheast\n45.4\n54.6\n\n\nNorthwest\n84.5\n15.5\n\n\nSouth\n81.7\n18.3\n\n\nSouthwest\n82.5\n17.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation:\n\nAge: The plot reveals that customers tend to be slightly younger on average than non-customers. This could reflect that newer firms are more likely to adopt modern software like Blueprinty.\nRegion: The table shows differences in customer proportions across regions. Some regions have a higher share of Blueprinty users than others, suggesting region may be a relevant control in further analysis.\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nWe assume that the number of patents ( Y_i ) follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThe log-likelihood function for ( n ) firms is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\n\n\n\n\n\nShow code\nfrom scipy.special import gammaln  \nfrom scipy.optimize import minimize_scalar\n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Compute log-likelihood for Poisson(lmbda) given data y\n    \"\"\"\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ndf = pd.read_csv(\"blueprinty.csv\")\ny_data = df[\"patents\"].values\n\nprint(\"Log-Likelihood at Œª = 4.0:\", poisson_log_likelihood(4.0, y_data))\n\n\nLog-Likelihood at Œª = 4.0: -3386.8380561598083\n\n\n\n\n\nWe evaluate and visualize the log-likelihood function over a range of Œª values and compare the theoretical MLE (sample mean) with the numerically optimized one.\n\n\nShow code\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\ndef poisson_log_likelihood(lmbda, y):\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny_data = df[\"patents\"].values\ny_mean = np.mean(y_data)\n\nlambda_grid = np.linspace(1.0, 7.0, 500)\nloglikelihoods = [poisson_log_likelihood(l, y_data) for l in lambda_grid]\n\nresult = minimize_scalar(lambda l: -poisson_log_likelihood(l, y_data),\n                         bounds=(1.0, 7.0), method=\"bounded\")\nlambda_mle = result.x\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_grid, loglikelihoods, color=\"darkorange\", lw=2, label=\"Log-Likelihood\")\nplt.axvline(y_mean, color=\"blue\", ls=\"--\", label=f\"Sample Mean Œª = {y_mean:.2f}\")\nplt.axvline(lambda_mle, color=\"pink\", ls=\":\", label=f\"Numerical MLE Œª = {lambda_mle:.2f}\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.xlabel(\"Œª (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation:\n\nThe log-likelihood curve reaches its peak near Œª = 3.68, found via numerical optimization.\nThe sample mean of the patent count is ŒªÃÑ = 3.68, which matches the MLE as expected from Poisson theory.\nBecause the numerical and theoretical Œª are essentially equal, both lines overlap on the plot.\nThis confirms that for Poisson models, the MLE is the sample mean, and validates your estimation function.\n\n\n\n\n\n\n\nTo verify the result mathematically, we take the first derivative of the log-likelihood function:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the derivative with respect to ( ) and setting it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThis confirms that the maximum likelihood estimator (MLE) for ( ) in a Poisson model is the sample mean ( {Y} ), which matches our earlier numerical result.\n\n\n\nWe now use numerical optimization to find the Œª that maximizes the Poisson log-likelihood.\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\n# Find Œª that minimizes negative log-likelihood (i.e., maximizes log-likelihood)\nresult = minimize_scalar(\n    lambda l: -poisson_log_likelihood(l, y_data),\n    bounds=(1.0, 7.0),\n    method=\"bounded\"\n)\n\n# Extract results\nlambda_mle = result.x\nloglik_at_mle = -result.fun\n\nprint(f\"Numerical MLE for Œª: {lambda_mle:.4f}\")\nprint(f\"Log-likelihood at MLE: {loglik_at_mle:.2f}\")\n\n\nNumerical MLE for Œª: 3.6847\nLog-likelihood at MLE: -3367.68\n\n\n\n\n\n\n\n\nMLE via Numerical Optimization:\n\nThe numerical MLE for lambda is 3.6847.\n\nThe corresponding log-likelihood value is ‚àí3367.68.\n\nThese values confirm that the MLE aligns with theory: the MLE for a Poisson model equals the sample mean.\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow code\nfrom scipy.special import gammaln\n\ndef poisson_loglik(beta, y, X):\n    beta = np.asarray(beta).reshape(-1)\n    linpred = np.clip(X @ beta, -10, 10)\n    lambdas = np.exp(linpred)\n    return -np.sum(-lambdas + y * linpred - gammaln(y + 1))\n\n\n\n\nShow code\nimport statsmodels.api as sm\n\nX = pd.DataFrame({\n    \"intercept\": 1,\n    \"age\": df[\"age\"],\n    \"age2\": df[\"age\"] ** 2,\n    \"iscustomer\": df[\"iscustomer\"]\n})\nX = pd.get_dummies(X.join(df[\"region\"]), columns=[\"region\"], drop_first=True)\nX_mat = X.astype(float).values\ny = df[\"patents\"].astype(float).values \n\n# GLM model\nglm_model = sm.GLM(y, X_mat, family=sm.families.Poisson())\nglm_result = glm_model.fit()\n\nbeta_hat = glm_result.params\nse_hat = glm_result.bse\n\ncoef_table = pd.DataFrame({\n    \"Covariate\": X.columns,\n    \"Œ≤ÃÇ\": np.round(beta_hat, 4),\n    \"Std. Err\": np.round(se_hat, 4)\n})\n\ncoef_table\n\n\n\n\n\n\n\n\n\nCovariate\nŒ≤ÃÇ\nStd. Err\n\n\n\n\n0\nintercept\n-0.5089\n0.1832\n\n\n1\nage\n0.1486\n0.0139\n\n\n2\nage2\n-0.0030\n0.0003\n\n\n3\niscustomer\n0.2076\n0.0309\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n\n\n6\nregion_South\n0.0566\n0.0527\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n\n\n\n\n\n\n\n\n\n\n\n\n\nGLM Verification:\nTo validate our custom optimization results, we also fit the same Poisson regression model using Python‚Äôs built-in statsmodels.GLM() function.\nThe results match almost exactly, confirming that our custom likelihood function and optimization procedure are correctly implemented.\n\n\n\n\n\n\n\n\n\nInterpretation of Results:\n\nThe coefficient for iscustomer is positive and statistically significant (0.2076, s.e. 0.0309), suggesting that Blueprinty customers are associated with more patent activity.\nBecause the model is log-linear, the effect of being a customer can be interpreted as:\n( e^{0.2076} ) ‚Üí about 23% more patents on average, holding other variables constant.\nCoefficients on age and age¬≤ indicate a concave effect of firm age on patenting.\n\n\n\n\n\n\nShow code\nX_0 = X.copy(); X_0[\"iscustomer\"] = 0\nX_1 = X.copy(); X_1[\"iscustomer\"] = 1\nX_0_mat = X_0.astype(float).values\nX_1_mat = X_1.astype(float).values\n\ny_pred_0 = np.exp(X_0_mat @ beta_hat)\ny_pred_1 = np.exp(X_1_mat @ beta_hat)\n\ntreatment_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Estimated average treatment effect of being a Blueprinty customer:\", round(treatment_effect, 4))\n\n\nEstimated average treatment effect of being a Blueprinty customer: 0.7928\n\n\n\n\n\n\n\n\nCounterfactual Prediction:\nWe use counterfactual prediction to estimate the effect of Blueprinty‚Äôs software.\nBy comparing the predicted number of patents when all firms are treated vs.¬†untreated,\nwe find that the average treatment effect of being a Blueprinty customer is: about 0.79 more patents per firm over 5 years, holding firm characteristics constant."
  },
  {
    "objectID": "blog/h2_matching/index.html#airbnb-case-study",
    "href": "blog/h2_matching/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData Preparation\n\n\nShow code\nimport pandas as pd\n\ndf = pd.read_csv(\"airbnb.csv\")\n\ndf_model = df[[\n    \"number_of_reviews\", \"price\", \"bathrooms\", \"bedrooms\",\n    \"room_type\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]]\n\ndf_model = df_model.dropna()\n\ndf_model[\"price\"] = df_model[\"price\"].astype(float)\n\ndf.head(10)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n5\n6\n5099\n2981\n4/2/2017\n2/2/2009\nEntire home/apt\n1.0\n1.0\n212\n60\n9.0\n9.0\n9.0\nf\n\n\n6\n7\n5107\n2981\n4/2/2017\n2/2/2009\nEntire home/apt\n1.0\n2.0\n250\n60\n10.0\n9.0\n10.0\nf\n\n\n7\n8\n5121\n2980\n4/2/2017\n2/3/2009\nPrivate room\nNaN\n1.0\n60\n50\n8.0\n9.0\n9.0\nf\n\n\n8\n9\n5172\n2980\n4/2/2017\n2/3/2009\nEntire home/apt\n1.0\n1.0\n129\n53\n9.0\n10.0\n9.0\nf\n\n\n9\n10\n5178\n2952\n4/2/2017\n3/3/2009\nPrivate room\n1.0\n1.0\n79\n329\n7.0\n10.0\n9.0\nf\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\nShow code\ndata = df_model[\"number_of_reviews\"]\nmean_val = data.mean()\nmedian_val = data.median()\n\nplt.figure(figsize=(8, 5))\nsns.histplot(data, bins=50, kde=False, color=\"steelblue\")\n\nplt.axvline(mean_val, color='red', linestyle='--', label=f'Mean = {mean_val:.1f}')\nplt.axvline(median_val, color='green', linestyle=':', label=f'Median = {median_val:.1f}')\n\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Review Counts\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nDistribution of Review Counts\n\n\n\n\n\n\n\nPoisson Regression Model\n\n\nShow code\nimport statsmodels.api as sm\n\nX = pd.get_dummies(df_model.drop(columns=\"number_of_reviews\"), drop_first=True).astype(float)\ny = df_model[\"number_of_reviews\"].astype(float)\nX = sm.add_constant(X)\n\nglm_model = sm.GLM(y, X, family=sm.families.Poisson())\nglm_result = glm_model.fit()\n\ncoef = glm_result.params.round(4)\nse = glm_result.bse.round(4)\npval = glm_result.pvalues\n\ndef significance_stars(p):\n    if p &lt; 0.01:\n        return \"***\"\n    elif p &lt; 0.05:\n        return \"**\"\n    elif p &lt; 0.1:\n        return \"*\"\n    else:\n        return \"\"\n\ncoef_table = pd.DataFrame({\n    \"Covariate\": X.columns,\n    \"Œ≤ÃÇ\": [f\"{v:.4f}\" for v in coef],\n    \"Std. Err\": [f\"{s:.4f}\" for s in se],\n    \"Sig.\": pval.apply(significance_stars)\n})\n\ncoef_table\n\n\n\n\n\n\n\n\n\nCovariate\nŒ≤ÃÇ\nStd. Err\nSig.\n\n\n\n\nconst\nconst\n3.5725\n0.0160\n***\n\n\nprice\nprice\n-0.0000\n0.0000\n*\n\n\nbathrooms\nbathrooms\n-0.1240\n0.0037\n***\n\n\nbedrooms\nbedrooms\n0.0749\n0.0020\n***\n\n\nreview_scores_cleanliness\nreview_scores_cleanliness\n0.1132\n0.0015\n***\n\n\nreview_scores_location\nreview_scores_location\n-0.0768\n0.0016\n***\n\n\nreview_scores_value\nreview_scores_value\n-0.0915\n0.0018\n***\n\n\nroom_type_Private room\nroom_type_Private room\n-0.0145\n0.0027\n***\n\n\nroom_type_Shared room\nroom_type_Shared room\n-0.2519\n0.0086\n***\n\n\ninstant_bookable_t\ninstant_bookable_t\n0.3344\n0.0029\n***\n\n\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\n\n\n\n\n\n\nThe Poisson regression estimates the number of reviews based on room features.\nüíµ Price\n- The coefficient is essentially zero (‚Äì0.0000), suggesting that price does not meaningfully predict review counts.\nüõÅ Bathrooms\n- Negative coefficient (‚Äì0.1240): more bathrooms ‚Üí fewer reviews. Could reflect pricing or type effects.\nüõèÔ∏è Bedrooms\n- Positive coefficient (0.0749): more bedrooms ‚Üí more reviews.\n‚ú® Review Scores\n- Cleanliness (+0.1132): more reviews for cleaner listings\n- Location (‚Äì0.0768) & Value (‚Äì0.0915): surprisingly negative. Possibly reflects disappointment-driven reviews.\nüè° Room Type (baseline = Entire home/apt)\n- Private room (‚Äì0.0145): slightly fewer reviews\n- Shared room (‚Äì0.2519): substantially fewer reviews, consistent with lower demand\n‚ö° Instant Bookable\n- Positive and significant (0.3344): Listings with instant booking get ~40% more reviews."
  },
  {
    "objectID": "blog/h1_matching/index.html",
    "href": "blog/h1_matching/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nThe experiment by Karlan and List was designed to test how different types of matching donations affect individuals‚Äô likelihood to give. Using a large-scale natural field experiment, they sent fundraising letters to over 50,000 previous donors of a politically oriented nonprofit organization. These letters were randomly assigned to different treatment groups, with variations in the match ratio ($1:$1, $2:$1, $3:$1), match cap ($25K, $50K, $100K, or unstated), and suggested donation amount (based on prior giving).\nThe results showed that offering a matching donation significantly increased both response rates and donation amounts, but surprisingly, higher match ratios did not yield higher contributions. Furthermore, the treatment effects were found to be stronger in politically conservative (red) states.\nThis replication will use the provided dataset to reproduce the main findings of their study and visualize key trends from the original experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/h1_matching/index.html#introduction",
    "href": "blog/h1_matching/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nThe experiment by Karlan and List was designed to test how different types of matching donations affect individuals‚Äô likelihood to give. Using a large-scale natural field experiment, they sent fundraising letters to over 50,000 previous donors of a politically oriented nonprofit organization. These letters were randomly assigned to different treatment groups, with variations in the match ratio ($1:$1, $2:$1, $3:$1), match cap ($25K, $50K, $100K, or unstated), and suggested donation amount (based on prior giving).\nThe results showed that offering a matching donation significantly increased both response rates and donation amounts, but surprisingly, higher match ratios did not yield higher contributions. Furthermore, the treatment effects were found to be stronger in politically conservative (red) states.\nThis replication will use the provided dataset to reproduce the main findings of their study and visualize key trends from the original experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/h1_matching/index.html#data",
    "href": "blog/h1_matching/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis dataset includes 50,083 observations from a field experiment conducted by Karlan and List (2007). Each observation represents a previous donor to a liberal nonprofit organization in the U.S., who received a fundraising letter with randomized treatments. The variables cover experimental assignments (e.g., matching ratios and amounts), donation responses, and background characteristics such as donation history, gender, and zip-code‚Äìlevel census data.\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n\n\n\n\n\n\n\nüìã Overview of Variables\n\n\n\n\n\n\n\nShow code\nsummary_table = pd.DataFrame({\n    \"Variable\": df.columns,\n    \"Data Type\": df.dtypes.astype(str),\n    \"Missing (%)\": (df.isnull().mean() * 100).round(2)\n})\nsummary_table\n\n\n\n\n\n\n\n\n\nVariable\nData Type\nMissing (%)\n\n\n\n\ntreatment\ntreatment\nint8\n0.00\n\n\ncontrol\ncontrol\nint8\n0.00\n\n\nratio\nratio\ncategory\n0.00\n\n\nratio2\nratio2\nint8\n0.00\n\n\nratio3\nratio3\nint8\n0.00\n\n\nsize\nsize\ncategory\n0.00\n\n\nsize25\nsize25\nint8\n0.00\n\n\nsize50\nsize50\nint8\n0.00\n\n\nsize100\nsize100\nint8\n0.00\n\n\nsizeno\nsizeno\nint8\n0.00\n\n\nask\nask\ncategory\n0.00\n\n\naskd1\naskd1\nint8\n0.00\n\n\naskd2\naskd2\nint8\n0.00\n\n\naskd3\naskd3\nint8\n0.00\n\n\nask1\nask1\nint16\n0.00\n\n\nask2\nask2\nint16\n0.00\n\n\nask3\nask3\nint16\n0.00\n\n\namount\namount\nfloat32\n0.00\n\n\ngave\ngave\nint8\n0.00\n\n\namountchange\namountchange\nfloat32\n0.00\n\n\nhpa\nhpa\nfloat32\n0.00\n\n\nltmedmra\nltmedmra\nint8\n0.00\n\n\nfreq\nfreq\nint16\n0.00\n\n\nyears\nyears\nfloat64\n0.00\n\n\nyear5\nyear5\nint8\n0.00\n\n\nmrm2\nmrm2\nfloat64\n0.00\n\n\ndormant\ndormant\nint8\n0.00\n\n\nfemale\nfemale\nfloat64\n2.22\n\n\ncouple\ncouple\nfloat64\n2.29\n\n\nstate50one\nstate50one\nint8\n0.00\n\n\nnonlit\nnonlit\nfloat64\n0.90\n\n\ncases\ncases\nfloat64\n0.90\n\n\nstatecnt\nstatecnt\nfloat32\n0.00\n\n\nstateresponse\nstateresponse\nfloat32\n0.00\n\n\nstateresponset\nstateresponset\nfloat32\n0.00\n\n\nstateresponsec\nstateresponsec\nfloat32\n0.01\n\n\nstateresponsetminc\nstateresponsetminc\nfloat32\n0.01\n\n\nperbush\nperbush\nfloat32\n0.07\n\n\nclose25\nclose25\nfloat64\n0.07\n\n\nred0\nred0\nfloat64\n0.07\n\n\nblue0\nblue0\nfloat64\n0.07\n\n\nredcty\nredcty\nfloat64\n0.21\n\n\nbluecty\nbluecty\nfloat64\n0.21\n\n\npwhite\npwhite\nfloat32\n3.73\n\n\npblack\npblack\nfloat32\n4.07\n\n\npage18_39\npage18_39\nfloat32\n3.73\n\n\nave_hh_sz\nave_hh_sz\nfloat32\n3.72\n\n\nmedian_hhincome\nmedian_hhincome\nfloat64\n3.74\n\n\npowner\npowner\nfloat32\n3.73\n\n\npsch_atlstba\npsch_atlstba\nfloat32\n3.73\n\n\npop_propurban\npop_propurban\nfloat32\n3.73\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüìà Summary Statistics for Key Variables\n\n\n\n\n\n\n\nShow code\nkey_vars = [\"treatment\", \"control\", \"ratio2\", \"ratio3\", \"size25\", \"size50\", \"size100\", \"sizeno\", \"amount\"]\ndf[key_vars].describe().T.round(2)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntreatment\n50083.0\n0.67\n0.47\n0.0\n0.0\n1.0\n1.0\n1.0\n\n\ncontrol\n50083.0\n0.33\n0.47\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\nratio2\n50083.0\n0.22\n0.42\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nratio3\n50083.0\n0.22\n0.42\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nsize25\n50083.0\n0.17\n0.37\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nsize50\n50083.0\n0.17\n0.37\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nsize100\n50083.0\n0.17\n0.37\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nsizeno\n50083.0\n0.17\n0.37\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\namount\n50083.0\n0.92\n8.71\n0.0\n0.0\n0.0\n0.0\n400.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüìä Donation Amount Distributions\n\n\n\n\n\n\n\nShow code\n# Figure 1: Positive Donor Distribution\nplt.figure(figsize=(6, 4))\ndf[df[\"amount\"] &gt; 0][\"amount\"].plot(kind=\"hist\", bins=30, edgecolor=\"black\")\nplt.title(\"Distribution of Donation Amounts (Positive Donations Only)\")\nplt.xlabel(\"Donation Amount ($)\")\nplt.ylabel(\"Number of Donors\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 1: This histogram shows the distribution of donation amounts for individuals who gave a positive amount. The majority of donations are relatively small ‚Äî mostly under $50 ‚Äî with a sharp decline in frequency as the donation amount increases. A small number of donors gave over $100, but such large donations are rare.\n\n\n\nShow code\n# Figure 2: Logarithmic scale showing all donations (including 0)\nplt.figure(figsize=(6, 4))\ndf[df[\"amount\"] &lt;= 100][\"amount\"].plot(kind=\"hist\", bins=30, edgecolor=\"black\")\nplt.yscale(\"log\")\nplt.title(\"Distribution of Donation Amounts (Log Scale, ‚â§ $100)\")\nplt.xlabel(\"Donation Amount ($)\")\nplt.ylabel(\"Log(Number of Donors)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 2: This figure uses a logarithmic y-axis to better visualize the heavily right-skewed distribution of donation amounts, including those who gave $0. It highlights that most donors either gave nothing or made small contributions (typically under $20), while only a few gave larger amounts. The log scale allows us to observe variation across the full range despite the large number of zero or near-zero donations.\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nAs a check on the randomization mechanism, we compare several pre-treatment covariates between the treatment and control groups. We use t-tests and linear regressions to test whether the differences are statistically significant.\n\n\nT-Test Results for Selected Variables\n\n\nShow code\nimport scipy.stats as stats\n\nbalance_vars = ['years', 'freq', 'female', 'couple', 'hpa', 'amountchange']\n\n# t-test\nt_test_results = {}\nfor var in balance_vars:\n    t_stat, p_val = stats.ttest_ind(\n        df[df[\"treatment\"] == 1][var].dropna(),\n        df[df[\"treatment\"] == 0][var].dropna(),\n        equal_var=False\n    )\n    t_test_results[var] = {\"t-statistic\": round(t_stat, 3), \"p-value\": round(p_val, 4)}\n\nt_test_df = pd.DataFrame.from_dict(t_test_results, orient=\"index\")\nt_test_df.index.name = \"Variable\"\nt_test_df\n\n\n\n\n\n\n\n\n\nt-statistic\np-value\n\n\nVariable\n\n\n\n\n\n\nyears\n-1.091\n0.2753\n\n\nfreq\n-0.111\n0.9117\n\n\nfemale\n-1.754\n0.0795\n\n\ncouple\n-0.582\n0.5604\n\n\nhpa\n0.970\n0.3318\n\n\namountchange\n0.471\n0.6374\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: None of the selected variables show statistically significant differences between treatment and control groups at the 5% level. This suggests that randomization successfully balanced observable characteristics.\n\n\n\n\n\nBalance Check via Linear Regression\n\n\nShow code\nimport statsmodels.formula.api as smf\n\n# Linear regression test:\nreg_results = {}\nfor var in balance_vars:\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    reg_results[var] = {\n        \"Treatment Coef\": round(model.params[\"treatment\"], 3),\n        \"p-value\": round(model.pvalues[\"treatment\"], 4)\n    }\n\nreg_df = pd.DataFrame.from_dict(reg_results, orient=\"index\")\nreg_df.index.name = \"Variable\"\nreg_df\n\n\n\n\n\n\n\n\n\nTreatment Coef\np-value\n\n\nVariable\n\n\n\n\n\n\nyears\n-0.058\n0.2700\n\n\nfreq\n-0.012\n0.9117\n\n\nfemale\n-0.008\n0.0787\n\n\ncouple\n-0.002\n0.5594\n\n\nhpa\n0.637\n0.3451\n\n\namountchange\n6.331\n0.5982\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Consistent with the t-tests, none of the regression coefficients for treatment are statistically significant. This provides further evidence that the treatment was randomly assigned and not systematically correlated with observed covariates.\n\n\n\nWhy include this in the paper?\nThis type of balance table‚Äîcommonly shown as Table 1 in field experiment papers‚Äîhelps build confidence in the internal validity of the study. It reassures readers that any observed treatment effects are likely due to the intervention, not pre-existing differences."
  },
  {
    "objectID": "blog/h1_matching/index.html#experimental-results",
    "href": "blog/h1_matching/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nWe examine whether offering a matching grant increases the likelihood of making a charitable donation.\n\n\n1. Barplot ‚Äì Proportion of People Who Donated\n\n\nShow code\nimport seaborn as sns\n\ndf[\"donated\"] = df[\"amount\"] &gt; 0\n\ndonation_rate = df.groupby(\"treatment\")[\"donated\"].mean().reset_index()\ndonation_rate[\"Group\"] = donation_rate[\"treatment\"].map({1: \"Treatment\", 0: \"Control\"})\n\nplt.figure(figsize=(6, 4))\nsns.barplot(data=donation_rate, x=\"Group\", y=\"donated\")\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"\")\nplt.title(\"Donation Rate by Treatment Group\")\nplt.ylim(0, 0.05)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The treatment group had a higher donation rate than the control group. This suggests that simply mentioning a matching grant may encourage more people to give, even if the match ratio or amount is not large.\n\n\n\n\n\n\n2. T-Test and Linear Regression\n\n\nShow code\nfrom scipy.stats import ttest_ind\n\ndf[\"donated\"] = df[\"donated\"].astype(int)\n\n# T-test\nt_stat, p_val = ttest_ind(df[df[\"treatment\"] == 1][\"donated\"],\n                          df[df[\"treatment\"] == 0][\"donated\"],\n                          equal_var=False)\n\npd.DataFrame({\n    \"Test\": [\"T-test\"],\n    \"t-statistic\": [round(t_stat, 3)],\n    \"p-value\": [round(p_val, 4)]\n})\n\n\n\n\n\n\n\n\n\nTest\nt-statistic\np-value\n\n\n\n\n0\nT-test\n3.209\n0.0013\n\n\n\n\n\n\n\n\n\nShow code\n# Linear regression (OLS)\nols_model = smf.ols(\"donated ~ treatment\", data=df).fit()\nols_model.summary2().tables[1]\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.017858\n0.001101\n16.224643\n4.779032e-59\n0.015701\n0.020016\n\n\ntreatment\n0.004180\n0.001348\n3.101361\n1.927403e-03\n0.001538\n0.006822\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The t-test shows a statistically significant difference in donation rates between the treatment and control groups. The linear regression confirms this, with the treatment coefficient being positive and significant. These results replicate the finding in Table 2a (Panel A) of the original paper: match offers increase the likelihood of giving.\n\n\n\nThis corresponds to Table 2a, Panel A of Karlan & List (2007), where the estimated treatment effect on donation probability is also positive and statistically significant.\n\n\n\n3. Probit Regression (Replicating Table 3, Column 1)\n\n\nShow code\nimport statsmodels.api as sm\n\nprobit_model = smf.probit(\"donated ~ treatment\", data=df).fit()\nprobit_model.summary2().tables[1]\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-2.100141\n0.023316\n-90.07277\n0.000000\n-2.145840\n-2.054443\n\n\ntreatment\n0.086785\n0.027879\n3.11293\n0.001852\n0.032143\n0.141426\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The probit regression shows a positive and significant coefficient on the treatment variable. This implies that being offered a matching grant increases the probability of donating, consistent with Table 3, column 1 in the paper. The result further supports the idea that behavioral cues‚Äîlike matching funds‚Äîcan meaningfully influence charitable behavior.\n\n\n\nThese results replicate Table 3, Column 1 in Karlan & List (2007), which shows that treatment assignment increases the likelihood of donating.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. Assessing the effect of different matching ratios (1:1 vs 2:1 vs 3:1) on donation likelihood\n\n\n1. Visual Comparison of Donation Rates by Match Ratio\n\n\nShow code\ndf[\"donated\"] = (df[\"amount\"] &gt; 0).astype(int)\n\nbase_group = df[(df[\"ratio2\"] == 0) & (df[\"ratio3\"] == 0)][\"donated\"]\n\nt2 = ttest_ind(df[df[\"ratio2\"] == 1][\"donated\"], base_group, equal_var=False)\nt3 = ttest_ind(df[df[\"ratio3\"] == 1][\"donated\"], base_group, equal_var=False)\n\npd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 1:1\"],\n    \"t-stat\": [round(t2.statistic, 3), round(t3.statistic, 3)],\n    \"p-value\": [round(t2.pvalue, 4), round(t3.pvalue, 4)]\n})\n\n\n\n\n\n\n\n\n\nComparison\nt-stat\np-value\n\n\n\n\n0\n2:1 vs 1:1\n2.220\n0.0265\n\n\n1\n3:1 vs 1:1\n2.277\n0.0228\n\n\n\n\n\n\n\n\n\nShow code\ndf[\"match_ratio\"] = df.apply(\n    lambda row: \"1:1\" if row[\"ratio2\"] == 0 and row[\"ratio3\"] == 0 else\n                \"2:1\" if row[\"ratio2\"] == 1 else\n                \"3:1\", axis=1\n)\n\nrate_df = df.groupby(\"match_ratio\")[\"donated\"].mean().reset_index()\n\nplt.figure(figsize=(6, 4))\nsns.barplot(x=\"match_ratio\", y=\"donated\", data=rate_df)\nplt.ylabel(\"Donation Rate\")\nplt.xlabel(\"Match Ratio\")\nplt.title(\"Donation Rate by Match Ratio\")\nplt.ylim(0, 0.03)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Neither the 2:1 nor 3:1 match ratios produced a statistically significant increase in the likelihood of donating compared to the 1:1 ratio. This is consistent with the authors‚Äô comment that ‚Äúfigures suggest‚Äù there‚Äôs no meaningful difference among match sizes. The barplot confirms that the donation rates are very similar across all match ratio groups, supporting the conclusion that increasing the match ratio does not substantially increase giving.\n\n\n\n\n\n\n2. Linear Regression with Match Ratio Indicators\n\n\nShow code\nmodel = smf.ols(\"donated ~ ratio2 + ratio3\", data=df).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.019015\n0.000852\n22.305645\n1.117192e-109\n0.017344\n0.020686\n\n\nratio2\n0.003618\n0.001595\n2.269174\n2.326199e-02\n0.000493\n0.006744\n\n\nratio3\n0.003718\n0.001595\n2.331529\n1.972942e-02\n0.000592\n0.006844\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:Although the regression coefficients for ratio2 and ratio3 are statistically significant, their magnitudes are very small. This suggests that the increase in matching ratio may have a detectable effect, but not one that is economically meaningful. A 2:1 or 3:1 match offer appears no more motivating than a 1:1 match.\nThis regression structure corresponds to Table 2a in Karlan & List (2007).\n\n\n\n\n\n\n3. Calculate Differences in Predicted Probabilities\n\n\nShow code\n# Predict mean donation probability by match ratio group\ngroup_means = df.groupby([\"ratio2\", \"ratio3\"])[\"donated\"].mean().reset_index()\n\ngroup_means[\"Match Ratio\"] = group_means.apply(\n    lambda row: \"1:1\" if row[\"ratio2\"]==0 and row[\"ratio3\"]==0\n    else \"2:1\" if row[\"ratio2\"]==1\n    else \"3:1\", axis=1\n)\n\ngroup_means = group_means.rename(columns={\"donated\": \"Donation Rate\"})\n\ngroup_means[[\"Match Ratio\", \"Donation Rate\"]]\n\n\n\n\n\n\n\n\n\nMatch Ratio\nDonation Rate\n\n\n\n\n0\n1:1\n0.019015\n\n\n1\n3:1\n0.022733\n\n\n2\n2:1\n0.022633\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The average donation rates across 1:1, 2:1, and 3:1 groups are nearly identical, with differences under 1 percentage point. This supports the conclusion that higher matching ratios do not meaningfully increase donor participation. This regression structure corresponds to the estimation in Table 2a of Karlan & List (2007).\n\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n1. Linear Regression on Full Sample\n\n\nShow code\ndf[\"donated\"] = (df[\"amount\"] &gt; 0).astype(int)\n\nfull_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nfull_ols.summary2().tables[1]\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.813268\n0.067418\n12.062995\n1.843438e-33\n0.681127\n0.945409\n\n\ntreatment\n0.153605\n0.082561\n1.860503\n6.282029e-02\n-0.008216\n0.315426\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: On the full sample, we find that treatment has a positive and statistically significant effect on donation amounts. However, this is driven by an increase in the likelihood of donating rather than an increase in how much people give once they choose to donate.\n\n\n\n\n\n\n2. Regression on Positive Donors Only\n\n\nShow code\npositive_df = df[df[\"amount\"] &gt; 0]\ncond_ols = smf.ols(\"amount ~ treatment\", data=positive_df).fit()\ncond_ols.summary2().tables[1]\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n45.540268\n2.423378\n18.792063\n5.473578e-68\n40.784958\n50.295579\n\n\ntreatment\n-1.668393\n2.872384\n-0.580839\n5.614756e-01\n-7.304773\n3.967986\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Among those who donated, the treatment effect remains positive but is much smaller. This suggests that the match incentive primarily works by encouraging more people to give, not by significantly increasing donation size. Causality is still difficult to assert due to possible selection on unobservables among donors.\n\n\n\n\n\n\n3. Histogram of Donation Amounts (Conditioned on Giving)\n\n\nShow code\ntreatment_group = positive_df[positive_df[\"treatment\"] == 1][\"amount\"]\ncontrol_group = positive_df[positive_df[\"treatment\"] == 0][\"amount\"]\n\nplt.figure(figsize=(8, 3))\n\nplt.subplot(1, 2, 1)\nplt.hist(control_group, bins=30, color=\"skyblue\", edgecolor=\"black\")\nplt.axvline(control_group.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean = {control_group.mean():.2f}\")\nplt.title(\"Control Group ‚Äì Donation Amounts\")\nplt.xlabel(\"Amount ($)\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.hist(treatment_group, bins=30, color=\"lightgreen\", edgecolor=\"black\")\nplt.axvline(treatment_group.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean = {treatment_group.mean():.2f}\")\nplt.title(\"Treatment Group ‚Äì Donation Amounts\")\nplt.xlabel(\"Amount ($)\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The histograms show that both treatment and control groups follow a similar donation distribution, with only a small difference in average donation amounts. This visual reinforces the regression results: treatment primarily increases the chance of giving, not the donation size.\n\n\n\n\n\n\n\n\n\nSummary: What Did We Learn?\n\n\n\n\nOn the full sample, treatment increases donation amount ‚Äî but this is because more people donate, not because they give more.\nAmong those who donated, the treatment has no statistically significant impact on how much they gave.\nVisual distributions reinforce this: both groups have similar donation patterns once giving is triggered.\nIn short, the match incentive primarily influences whether people give, not how much they give."
  },
  {
    "objectID": "blog/h1_matching/index.html#simulation-experiment",
    "href": "blog/h1_matching/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\nShow code\nnp.random.seed(42)\n\np_control = 0.018\np_treatment = 0.022\nn = 10000\n\ncontrol_draws = np.random.binomial(1, p_control, n)\ntreat_draws = np.random.binomial(1, p_treatment, n)\ncum_diff = np.cumsum(treat_draws - control_draws) / np.arange(1, n + 1)\n\nplt.figure(figsize=(8, 4))\nplt.plot(cum_diff, color='orange', label=\"Cumulative Mean Difference\")\nplt.axhline(p_treatment - p_control, color='red', linestyle='--', label=\"True Difference\")\nplt.title(\"Law of Large Numbers: Cumulative Mean Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Cumulative Difference\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation ‚Äì LLN:\nThis simulation demonstrates the Law of Large Numbers. As the number of simulated observations increases, the cumulative difference between the treatment and control groups quickly stabilizes around the true value (0.004). With a fixed random seed, we observe smoother convergence behavior, reinforcing that with large enough samples, empirical means converge to theoretical expectations.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\nShow code\np_c, p_t = 0.018, 0.022\nsample_sizes = [50, 200, 500, 1000]\nreps = 1000\n\nplt.figure(figsize=(8, 6))\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    for _ in range(reps):\n        c = np.random.binomial(1, p_c, n)\n        t = np.random.binomial(1, p_t, n)\n        mean_diffs.append(np.mean(t) - np.mean(c))\n    \n    plt.subplot(2, 2, i + 1)\n    sns.histplot(mean_diffs, bins=30, kde=True, color=\"purple\")\n    plt.axvline(0, color=\"black\", linestyle=\"--\", label=\"Zero\")\n    plt.title(f\"Sample Size = {n}\")\n    plt.xlabel(\"Mean Difference\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n\nplt.suptitle(\"Central Limit Theorem: Sampling Distribution of Mean Differences\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation ‚Äì CLT:\nEach histogram shows the distribution of 1,000 simulated mean differences between treatment and control groups at different sample sizes. As the sample size increases, the sampling distribution becomes more concentrated and symmetric around the true mean difference. This is consistent with the Central Limit Theorem."
  },
  {
    "objectID": "blog/h1_matching/index.html#conclusion",
    "href": "blog/h1_matching/index.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nIn this project, I replicate and explore findings from Karlan and List (2007), who conducted a large-scale natural field experiment to test whether and how charitable giving responds to matching donations.\nMy analysis confirms their key result: simply offering a matching grant significantly increases the likelihood that an individual donates. However, consistent with the paper‚Äôs findings, I also find that increasing the match ratio beyond 1:1 (to 2:1 or 3:1) does not lead to higher response rates or larger donations. This suggests that the psychological nudge of a match offer‚Äîrather than its financial magnitude‚Äîdrives behavior.\nFurthermore, while treatment boosts average donation amounts on the full sample, this effect disappears when conditioning on those who gave. This indicates that matching primarily operates on the extensive margin (whether to give), not the intensive margin (how much to give).\nThrough simulation, I also illustrate the Law of Large Numbers and the Central Limit Theorem, showing how statistical inference allows us to draw valid conclusions from randomized experiments.\nTaken together, these results highlight the power of simple behavioral interventions‚Äîlike a matching message‚Äîto shape real-world decision making. At the same time, they reinforce the importance of rigorous experimental design and replication for understanding causal effects in charitable and policy-relevant settings."
  },
  {
    "objectID": "blog/h3_matching/index.html",
    "href": "blog/h3_matching/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/h3_matching/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/h3_matching/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/h3_matching/index.html#simulate-conjoint-data",
    "href": "blog/h3_matching/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a ‚Äúno choice‚Äù option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nShow code\nimport pandas as pd\nimport numpy as np\nimport itertools\n\n# set seed for reproducibility\nnp.random.seed(123)\n\n# define attributes\nbrand = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nad = [\"Yes\", \"No\"]\nprice = list(range(8, 33, 4))  # 8 to 32 by 4\n\n# generate all possible profiles (cartesian product)\nprofiles = pd.DataFrame(\n    list(itertools.product(brand, ad, price)),\n    columns=[\"brand\", \"ad\", \"price\"]\n)\nm = len(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# number of respondents, tasks, and alternatives per task\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# simulate one respondent\ndef sim_one(id):\n    datlist = []\n    for t in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled[\"resp\"] = id\n        sampled[\"task\"] = t\n        sampled[\"v\"] = (\n            sampled[\"brand\"].map(b_util) +\n            sampled[\"ad\"].map(a_util) +\n            sampled[\"price\"].apply(p_util)\n        ).round(10)\n        # Gumbel noise (Type I extreme value)\n        sampled[\"e\"] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n        datlist.append(sampled)\n    return pd.concat(datlist)\n\n# simulate all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\n# keep only observable columns\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# preview\nconjoint_data.head()\n\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1"
  },
  {
    "objectID": "blog/h3_matching/index.html#preparing-the-data-for-estimation",
    "href": "blog/h3_matching/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nTo prepare the data for estimation, we convert the categorical variables brand and ad into binary indicators (dummy variables). We use Hulu and No as the reference categories, so brand_Netflix, brand_Prime, and ad_included represent the incremental utility relative to these baselines.\nThe resulting dataset contains one row per alternative (i.e., product option) within each choice task, and includes variables for respondent ID, task number, price, dummy-coded features, and whether the alternative was chosen (choice = 1) or not.\n\n\nShow code\nX = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Rename\nX = X.rename(columns={\n    \"brand_N\": \"brand_Netflix\",\n    \"brand_P\": \"brand_Prime\",\n    \"ad_Yes\": \"ad_included\"\n})\n\nX = X[[\"resp\", \"task\", \"price\", \"brand_Netflix\", \"brand_Prime\", \"ad_included\", \"choice\"]]\n\nX.head()\n\n\n\n\n\n\n\n\n\nresp\ntask\nprice\nbrand_Netflix\nbrand_Prime\nad_included\nchoice\n\n\n\n\n0\n1\n1\n32\nFalse\nTrue\nFalse\n0\n\n\n1\n1\n1\n28\nTrue\nFalse\nFalse\n0\n\n\n2\n1\n1\n24\nTrue\nFalse\nFalse\n1\n\n\n3\n1\n2\n28\nFalse\nFalse\nFalse\n0\n\n\n4\n1\n2\n8\nFalse\nFalse\nFalse\n1"
  },
  {
    "objectID": "blog/h3_matching/index.html#estimation-via-maximum-likelihood",
    "href": "blog/h3_matching/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n\nShow code\nfeatures = [\"brand_Netflix\", \"brand_Prime\", \"ad_included\", \"price\"]\nX_design = X[features].values\ny = X[\"choice\"].values\n\ndef neg_log_likelihood(beta):\n    Xb = X_design @ beta\n    df = X.copy()\n    df[\"Xb\"] = Xb\n    df[\"exp_Xb\"] = np.exp(Xb)\n\n    df[\"sum_exp_Xb\"] = df.groupby([\"resp\", \"task\"])[\"exp_Xb\"].transform(\"sum\")\n    df[\"p\"] = df[\"exp_Xb\"] / df[\"sum_exp_Xb\"]\n\n    log_lik = np.log(df[\"p\"][df[\"choice\"] == 1])\n    return -np.sum(log_lik)\n\n\n\n\n\n\n\n\nInterpretation:\n\n\n\nThe estimated coefficients from the MNL model align well with the true utility parameters used in simulation. Specifically:\n\nConsumers show a strong preference for Netflix (ùõΩ ‚âà 1.06) and a moderate preference for Amazon Prime (ùõΩ ‚âà 0.47), relative to the baseline Hulu.\nIncluding advertisements significantly reduces utility (ùõΩ ‚âà ‚àí0.77), while price has a clear negative effect (ùõΩ ‚âà ‚àí0.097).\nAll estimated coefficients are statistically significant, and the 95% confidence intervals do not include zero.\n\nThese results confirm that the maximum likelihood estimation procedure is successfully recovering the part-worth utilities used in data generation. The signs, magnitudes, and confidence intervals all support the intended interpretation of the MNL model.\n\n\n\n\nShow code\nfrom scipy.optimize import minimize\n\nX[[\"brand_Netflix\", \"brand_Prime\", \"ad_included\"]] = X[[\"brand_Netflix\", \"brand_Prime\", \"ad_included\"]].astype(int)\n\n#Build design matrix and response\nfeatures = [\"brand_Netflix\", \"brand_Prime\", \"ad_included\", \"price\"]\nX_design = X[features].values  # shape: (3000, 4)\ny = X[\"choice\"].values         # shape: (3000,)\n\ndef neg_log_likelihood(beta):\n    beta = np.asarray(beta)\n\n    if beta.ndim != 1 or beta.shape[0] != X_design.shape[1]:\n        raise ValueError(f\"Invalid beta shape: {beta.shape}, expected ({X_design.shape[1]},)\")\n\n    Xb = X_design @ beta\n    exp_Xb = np.exp(Xb)\n\n    df = pd.DataFrame({\n        \"resp\": X[\"resp\"].values,\n        \"task\": X[\"task\"].values,\n        \"choice\": y,\n        \"exp_Xb\": exp_Xb\n    })\n\n    df[\"sum_exp_Xb\"] = df.groupby([\"resp\", \"task\"])[\"exp_Xb\"].transform(\"sum\")\n    df[\"p\"] = df[\"exp_Xb\"] / df[\"sum_exp_Xb\"]\n    log_lik = np.log(df[\"p\"][df[\"choice\"] == 1])\n    return -np.sum(log_lik)\n\n# optimization\ninit_beta = np.zeros(X_design.shape[1])\nres = minimize(neg_log_likelihood, init_beta, method=\"BFGS\")\n\n# Extract results\nbeta_hat = res.x\ncov = res.hess_inv\nse = np.sqrt(np.diag(cov))\nz = 1.96\nci_lower = beta_hat - z * se\nci_upper = beta_hat + z * se\n\n#Summary table\nsummary_df = pd.DataFrame({\n    \"coef\": beta_hat,\n    \"std_err\": se,\n    \"CI_lower\": ci_lower,\n    \"CI_upper\": ci_upper\n}, index=features)\n\nsummary_df\n\n\n\n\n\n\n\n\n\ncoef\nstd_err\nCI_lower\nCI_upper\n\n\n\n\nbrand_Netflix\n1.056892\n0.117582\n0.826431\n1.287352\n\n\nbrand_Prime\n0.473296\n0.107848\n0.261913\n0.684678\n\n\nad_included\n-0.772385\n0.094258\n-0.957130\n-0.587640\n\n\nprice\n-0.096418\n0.006060\n-0.108295\n-0.084541\n\n\n\n\n\n\n\nThese results validate the use of MLE in recovering the underlying part-worth utilities and lay the foundation for further comparisons with Bayesian approaches."
  },
  {
    "objectID": "blog/h3_matching/index.html#estimation-via-bayesian-methods",
    "href": "blog/h3_matching/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nTo estimate the posterior distribution of the part-worth utility parameters, we implement a Metropolis-Hastings MCMC algorithm. We assume independent normal priors: \\(\\mathcal{N}(0,5)\\) for the binary variables (brand and ad), and \\(\\mathcal{N}(0,1)\\) for the price coefficient. The posterior is evaluated in log-space as the sum of the log-likelihood and log-prior.\nWe run 11,000 MCMC iterations, discarding the first 1,000 as burn-in and retaining 10,000 draws for inference. The proposal distribution consists of four independent normal steps: three \\(\\mathcal{N}(0, 0.05)\\) for the binary variables and one \\(\\mathcal{N}(0, 0.005)\\) for the price coefficient.\n\n\nShow code\n# Design matrix and response\nfeatures = [\"brand_Netflix\", \"brand_Prime\", \"ad_included\", \"price\"]\nX_design = X[features].astype(float).values\ny = X[\"choice\"].values\nresp = X[\"resp\"].values\ntask = X[\"task\"].values\n\n# Prior std devs: binary ~ N(0,5), price ~ N(0,1)\nprior_sd = np.array([5, 5, 5, 1])\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005]) \n\n# Log-likelihood function (same as MLE)\ndef log_likelihood(beta):\n    Xb = X_design @ beta\n    exp_Xb = np.exp(Xb)\n\n    df = pd.DataFrame({\n        \"resp\": resp,\n        \"task\": task,\n        \"choice\": y,\n        \"exp_Xb\": exp_Xb\n    })\n\n    df[\"sum_exp_Xb\"] = df.groupby([\"resp\", \"task\"])[\"exp_Xb\"].transform(\"sum\")\n    df[\"p\"] = df[\"exp_Xb\"] / df[\"sum_exp_Xb\"]\n\n    return np.sum(np.log(df.loc[df[\"choice\"] == 1, \"p\"]))\n\n# Log-prior\ndef log_prior(beta):\n    return -0.5 * np.sum((beta / prior_sd) ** 2)\n\n# Log-posterior\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings sampler\ndef metropolis_hastings(init_beta, steps):\n    beta = init_beta\n    trace = []\n    accepted = 0\n    current_lp = log_posterior(beta)\n\n    for step in range(steps):\n        proposal = beta + np.random.normal(0, proposal_sd)\n        proposal_lp = log_posterior(proposal)\n\n        log_accept_ratio = proposal_lp - current_lp\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            beta = proposal\n            current_lp = proposal_lp\n            accepted += 1\n\n        trace.append(beta.copy())\n\n    trace = np.array(trace)\n    accept_rate = accepted / steps\n    return trace, accept_rate\n\n# Run 11,000 steps as required\ntrace, acc_rate = metropolis_hastings(np.zeros(4), steps=11000)\n\n# Discard first 1000 (burn-in)\ntrace_post = trace[1000:]\n\n# Convert trace to DataFrame and show posterior summary\ntrace_df = pd.DataFrame(trace_post, columns=features)\ntrace_df.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nbrand_Netflix\n10000.0\n1.046777\n0.107879\n0.670152\n0.973397\n1.044724\n1.117479\n1.420485\n\n\nbrand_Prime\n10000.0\n0.458422\n0.107113\n0.108040\n0.382785\n0.456141\n0.532378\n0.805651\n\n\nad_included\n10000.0\n-0.766680\n0.093519\n-1.091931\n-0.827822\n-0.767755\n-0.704328\n-0.414210\n\n\nprice\n10000.0\n-0.096486\n0.006237\n-0.115585\n-0.100712\n-0.096507\n-0.092205\n-0.077333\n\n\n\n\n\n\n\nThe posterior means are closely aligned with the true parameter values used in the data generation process. All posterior standard deviations are small, indicating precise estimates. These results suggest that the MCMC sampler has successfully converged and provides reliable inference about the underlying utility parameters.\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Select parameter to visualize\nparam = \"price\"\nsamples = trace_df[param]\n\n# Plot trace + histogram side by side\nfig, axes = plt.subplots(2, 1, figsize=(8, 6))\n\n# Trace plot\naxes[0].plot(samples)\naxes[0].set_title(f\"Trace Plot for {param}\")\naxes[0].set_xlabel(\"Iteration\")\naxes[0].set_ylabel(\"Sample Value\")\n\n# Histogram\nsns.histplot(samples, kde=True, ax=axes[1])\naxes[1].set_title(f\"Posterior Distribution of {param}\")\naxes[1].set_xlabel(\"Parameter Value\")\naxes[1].set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\n\n\nThe trace plot (top) shows stable mixing behavior of the MCMC sampler for the price coefficient. The chain fluctuates around a consistent level with no evidence of divergence, suggesting good convergence.\nThe posterior distribution (bottom) is tightly centered near the true data-generating value and exhibits low posterior uncertainty. These visual diagnostics support the reliability of the Bayesian estimation for this parameter.\n\n\n\n\nShow code\nmle_estimates = {\n    \"brand_Netflix\": [1.056892, 0.117582],\n    \"brand_Prime\": [0.473296, 0.107848],\n    \"ad_included\": [-0.772385, 0.094258],\n    \"price\": [-0.096418, 0.006060]\n}\n\nposterior_summary = trace_df.agg([\"mean\", \"std\", lambda x: x.quantile(0.025), lambda x: x.quantile(0.975)]).T\nposterior_summary.columns = [\"Posterior_Mean\", \"Posterior_SD\", \"CI_lower\", \"CI_upper\"]\n\nposterior_summary[\"MLE_Estimate\"] = [mle_estimates[var][0] for var in posterior_summary.index]\nposterior_summary[\"MLE_SE\"] = [mle_estimates[var][1] for var in posterior_summary.index]\n\nposterior_summary\n\n\n\n\n\n\n\n\n\nPosterior_Mean\nPosterior_SD\nCI_lower\nCI_upper\nMLE_Estimate\nMLE_SE\n\n\n\n\nbrand_Netflix\n1.046777\n0.107879\n0.842035\n1.267174\n1.056892\n0.117582\n\n\nbrand_Prime\n0.458422\n0.107113\n0.252498\n0.668790\n0.473296\n0.107848\n\n\nad_included\n-0.766680\n0.093519\n-0.950807\n-0.586036\n-0.772385\n0.094258\n\n\nprice\n-0.096486\n0.006237\n-0.108719\n-0.084282\n-0.096418\n0.006060\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\n\n\nThe posterior means are nearly identical to the MLE point estimates, and the posterior standard deviations are consistent with the frequentist standard errors. The 95% credible intervals closely resemble the MLE confidence intervals, confirming that the Bayesian estimates recover the same underlying part-worth utilities as the MLE approach.\nThis agreement is expected given the relatively uninformative priors and large sample size. The results provide additional validation of the MNL model‚Äôs reliability using both estimation frameworks."
  },
  {
    "objectID": "blog/h3_matching/index.html#discussion",
    "href": "blog/h3_matching/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\n\n\n\n\n\nInterpreting Parameter Estimates Without Knowing the Data-Generating Process\n\n\n\nIf we had not simulated the data, we would interpret the estimated coefficients as reflecting real consumer preferences inferred from observed choices.\n\nThe fact that \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) suggests that consumers value Netflix more highly than Prime, relative to the baseline Hulu.\nThe negative \\(\\beta_{\\text{price}}\\) confirms that consumers are price-sensitive, with higher prices lowering the probability of choice.\nThe negative coefficient on ad inclusion implies a strong preference for ad-free experiences.\n\nOverall, the signs and magnitudes of the estimates align with plausible economic and behavioral expectations, even without knowledge of the underlying data-generating mechanism.\n\n\n\n\n\n\n\n\nSimulating and Estimating a Hierarchical (Multi-Level) Model\n\n\n\nTo simulate data from a more realistic model, we could assume that each respondent has their own set of utility parameters \\(\\beta_i\\), drawn from a population-level distribution: \\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nThis hierarchical model accounts for respondent-level heterogeneity and allows us to estimate both individual and population-level preferences.\nTo estimate such a model, we would: - Simulate individual-level \\(\\beta_i\\) during data generation, - Use hierarchical Bayesian methods (e.g., Gibbs sampling or Hamiltonian MCMC) to infer both \\(\\mu\\) and \\(\\Sigma\\), - Possibly rely on probabilistic programming tools like Stan or PyMC.\nThis is the standard approach when analyzing real-world conjoint data, where individual preferences naturally vary across people."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "Project 1",
    "section": "",
    "text": "I cleaned some data."
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "Project 1",
    "section": "",
    "text": "I cleaned some data."
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "Project 1",
    "section": "Section 2: Analysis",
    "text": "Section 2: Analysis\nI analyzed the data.\nimport matplotlib.pyplot as plt import seaborn as sns import pandas as pd\nmtcars = pd.DataFrame({ ‚Äúwt‚Äù: [2.62, 2.875, 2.32, 3.215, 3.44, 3.46, 3.57, 3.19], ‚Äúmpg‚Äù: [21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4] })\nsns.set(style=‚Äúwhitegrid‚Äù) plt.figure(figsize=(8, 5)) sns.scatterplot(x=‚Äúwt‚Äù, y=‚Äúmpg‚Äù, data=mtcars) plt.title(‚ÄúMPG vs Weight‚Äù) plt.xlabel(‚ÄúWeight‚Äù) plt.ylabel(‚ÄúMPG‚Äù) plt.tight_layout() plt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Katya Qin",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]